
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Transformer Activation Tools</title>

    <meta name="description" content="Transformer Activation Tools: Tools for exploring Transformer neuron behaviour, including input pruning and diversification">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://transformer-activation-tools.pages.dev//img/graph_10_examples_n_20.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://transformer-activation-tools.pages.dev/"/>
    <meta property="og:title" content="Transformer Activation Tools" />
    <meta property="og:description" content="Project page for Transformer Activation Tools: Tools for exploring Transformer neuron behaviour, including input pruning and diversification." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="SayCan" />
    <meta name="twitter:description" content="Project page for Transformer Activation Tools: Tools for exploring Transformer neuron behaviour, including input pruning and diversification." />
    <meta name="twitter:image" content="https://transformer-activation-tools.pages.dev//img/graph_10_examples_n_20.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b><font size="+6">Transformer Activation Tools: </font></b>: </br> Tools for exploring Transformer neuron behaviour, including input pruning and diversification

            </br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Anonymous authors*</li>
                <br><br>
                    <!-- <a href="http://g.co/robotics">
                    <image src="img/robotics-at-google.png" height="40px"> Apart Research</a>
                    <a href="https://everydayrobots.com">
                    <image src="img/EverydayRobots2.gif" height="40px"> Everyday Robots</a> <br><br> -->
                    * Authors anonymized for review
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="" height="60px" width="40px">
                                <h4><strong>Paper (not available yet)</strong></h4>
                            </a>
                        </li>
                    <!-- <li>
                            <a href="https://youtu.be/ysFav0b472w">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <image src="img/new.png" height="20px" class="imtip">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li> 
                         <li>
                            <a href="https://github.com/google-research/google-research/tree/master/saycan">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>

         <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
           
                <h3>
                    What's New
                </h3>
                <p class="text-justify">

                <ul>
                    <li> <font color="#5a00b4">[8/16/2022]</font> We integrated SayCan with <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">Pathways Language Model (PaLM)</a>, and updated the results. We also added <a href="#new-capability"> new capabilities</a> including drawer manipulation, chain of thought prompting and multilingual instructions. You can see all the new results in the updated <a href="assets/palm_saycan.pdf">paper</a>.</li>
                    <li><font color="#5a00b4">[8/16/2022]</font> Our updated results show that SayCan combined with the improved language model (PaLM), which we refer to as PaLM-SayCan, improves the <b>robotics performance</b> of the entire system compared to a previous LLM (FLAN). PaLM-SayCan chooses the correct sequence of skills 84% of the time and executes them successfully 74% of the time, reducing errors by a half compared to FLAN.  This is particularly exciting because it represents the first time we can see how an improvement in language models translates to a similar improvement in robotics.  </li>
                    <li><font color="#5a00b4">[8/16/2022]</font> We <a href="#open-source">open-sourced</a> a version of SayCan on a simulated tabletop environment. </li>
                    <li> [4/4/2022] Initial release of SayCan. </li>
                </ul>
            </div>
        </div> -->
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <p style="text-align:center;">
        	    	<video id="v0" width="100%" playsinline autoplay muted loop controls>
                       <source src="img/demo_sequence_compressed.mp4" type="video/mp4">
                   </video>
                </p> -->
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Understanding the function of MLP neurons within Language Models is essen-
tial for mechanistic interpretability research. Unlike Vision Models, there are no
feature visualization techniques available, so this work relies on analyzing token-
level activations of a neuron on highly-activating dataset examples. These exam-
ples often contain irrelevant information and lack variety. While the importance
of highly-activating tokens may be clear, the influence of other tokens with lower
activation can only be inferred by looking for correlations across many exam-
ples. This work seeks to provide a better understanding of MLP neurons within
Language Models by analyzing token-level activations of a neuron on highly-
activating dataset examples. To increase the variety and relevance of these exam-
ples, we present tools for automatically generating additional inputs from a seed
dataset. These tools include pruning extraneous information, computing token
saliency to understand which tokens are important for neuron activation, and gen-
erating counterfactual examples by replacing one or more important tokens with
other high-probability substitutes using BERT. We accelerates interpretability re-
search by making neurons easier to interpret, providing a step towards automated
neuron analysis in neural networks.
                </p>
            </div>
        </div>


	<!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/Th6vwOtUt3k" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
Approach
                </h3>
                <p class="text-justify">
                    We begin by introducing the prune function, which takes a prompt with a maximally activating token
                    and finds a subset of the prompt with the highest possible neuron activation for the key token. We
                    then measure the importance of each token, and the relative activation of each token in a generated
                    set of prompts, which can then be used to visualise the neuron’s response
                    The prune function takes a prompt with the highest neuron activation for a particular neuron on
                    the key token, and then splits the prompt into sentences. It then takes all sentences up to and
                    including the sentence containing the token with the highest neuron activation and creates a new,
                    pruned prompt. We then truncate the smaller context of the prompt up to the key token, and measure
                    the activation of the neuron on the key token to determine the change in activation. If the change
                    is a decrement larger than a user-defined threshold, typically around -50%, then the prior token is
                    added to the truncated prompt. This process is then repeated until the activation of the key token is
                    sufficient to pass the condition.
                    Importance and relative activation are then computed for each token in a prompt. The importance
                    Ik of the kth token is calculated as Ik = 1 − (amasked/akey ), where amasked is the activation of the
                    neuron on the key token when token k is masked, and akey is the activation of the neuron on the key
                    token on the unmasked prompt. The relative activation R of a token is calculated as R = ak/akey ,
                    and is used to visualise the neuron’s response.
                    Finally, the pruned prompt is used to generate more varied inputs to explore the neuron’s activations.
                    This is done by masking each important token in turn and generating likely substitutions for that
                    token, creating new prompts with each of the top n substitutes, provided they cross a probability
                    threshold. The new prompts then have the importance and relative activation computed, and the
                    resulting visualisation provides insights into the neuron’s behaviour.          </div>  

<p style="text-align:center;">
    <image src="img/Example.png"  class="img-responsive" height="600px">
</p>

        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
		<p class="text-justify">
		We found that when we ran the code, the results came out.
		</p>
		
                <p style="text-align:center;">
                    <image src="img/graph_10_examples_n_20.png"  class="img-responsive" height="600px">
                </p>
	        </div>
        </div> -->


         <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3> <a href="https://arxiv.org/abs/2204.01691">[arxiv version]</a>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{acl2023interpretllmn,
    title={Interpreting Large Language Model Neurons},
    author={Anonymous authors},
    booktitle={ACL submission},
    year={2023}
}</textarea>
                </div>
            </div>
             
        </div> -->


         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <font color="#5a00b4">Models used in the paper</font>
                </h3>
               <font color="#5a00b4">The models are available...<a href="https://huggingface.co/gpt2">[gpt-2]</a></font>
            </div>
            <br/><br/>
        </div>


         <!-- <div class="row">
            <div id="open-source" class="col-md-8 col-md-offset-2">
                <h3>
                    <font color="#5a00b4">Open Source</font>
                </h3>
              <font color="#5a00b4">We open source a version of SayCan that works with a simulated tabletop environment. <a href="https://github.com/google-research/google-research/tree/master/saycan">[tabletop saycan] </a> </font>
              <p style="text-align:center;">
                    <img src="img/open_source_tabletop.png" class="img-responsive" height="600px">
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to thank Fred Alcober, Yunfei Bai, Matt Bennice, Maarten Bosma, Justin Boyd, Bill Byrne, Kendra Byrne, Noah Constant, Pete Florence, Laura Graesser, Rico Jonschkowski, Daniel Kappler, Hugo Larochelle, Benjamin Lee, Adrian Li, Maysam Moussalem, Suraj Nair, Jane Park, Evan Rapoport, Krista Reymann, Jeff Seto, Dhruv Shah, Ian Storz, Razvan Surdulescu, Tom Small, Jason Wei, and Vincent Zhao for their help and support in various aspects of the project.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div> -->
    </div>
    <!-- 100% privacy friendly analytics -->
<script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript>
</body>
</html>
