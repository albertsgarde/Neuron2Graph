{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyP0uYGKdXRapeYr4T7+o1z1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4b6e3059a2d1419d959f15e8b25cc83c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6aa4839f72e40efa568688bea920048","IPY_MODEL_7ad7e3b4abf74d3da4d85d713cf689bd","IPY_MODEL_7ee80cbeda3c4f8693dca4bd8ee3b094"],"layout":"IPY_MODEL_500bd098f31c41d2a8e22d47ab3a4722"}},"c6aa4839f72e40efa568688bea920048":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5507da728a456db8da86548b36dc98","placeholder":"​","style":"IPY_MODEL_43e9960b13ea4e5fb9e55e7e29bdb3c3","value":"Downloading (…)lve/main/config.json: 100%"}},"7ad7e3b4abf74d3da4d85d713cf689bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49c2ce1615f04fd78a11c7b00c38d809","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4165d6a45d845ffa133c73075542228","value":483}},"7ee80cbeda3c4f8693dca4bd8ee3b094":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_891f48d84fc946e7919d38699f0992b7","placeholder":"​","style":"IPY_MODEL_23ef0d518f4d48b3a40df32db3785c19","value":" 483/483 [00:00&lt;00:00, 23.4kB/s]"}},"500bd098f31c41d2a8e22d47ab3a4722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc5507da728a456db8da86548b36dc98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43e9960b13ea4e5fb9e55e7e29bdb3c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49c2ce1615f04fd78a11c7b00c38d809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4165d6a45d845ffa133c73075542228":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"891f48d84fc946e7919d38699f0992b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23ef0d518f4d48b3a40df32db3785c19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ff6b8142d2f409fb1b7576a11d08f61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6616deab6ee74b64b04a2a779098130e","IPY_MODEL_640c02e25d5e4ef0aa5ecbe7f2835f11","IPY_MODEL_5fdf00eb7f31407fbb096a223cdd8aeb"],"layout":"IPY_MODEL_eb87ac0a072647d18f7cb8b2b5ee06eb"}},"6616deab6ee74b64b04a2a779098130e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7faf98a913cb41e08e1fe978b650cc20","placeholder":"​","style":"IPY_MODEL_fa1f018adbde4b09bfe5c662c6552c98","value":"Downloading model.safetensors: 100%"}},"640c02e25d5e4ef0aa5ecbe7f2835f11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb07d48ceb3d4fae9e176639f6e534b3","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd9a8e65e8334ebc9e4b01ad707dc5bc","value":267954768}},"5fdf00eb7f31407fbb096a223cdd8aeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba24f1a604a14fbab0a2706ac257a5b6","placeholder":"​","style":"IPY_MODEL_6fc636d9a86641a88ebd6697cf1b2177","value":" 268M/268M [00:01&lt;00:00, 251MB/s]"}},"eb87ac0a072647d18f7cb8b2b5ee06eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7faf98a913cb41e08e1fe978b650cc20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa1f018adbde4b09bfe5c662c6552c98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb07d48ceb3d4fae9e176639f6e534b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9a8e65e8334ebc9e4b01ad707dc5bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba24f1a604a14fbab0a2706ac257a5b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fc636d9a86641a88ebd6697cf1b2177":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55b27838da544005ba26ae95a8eb5ca4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59a5c43fc5d94544a07ea8b64809ed10","IPY_MODEL_e9de6761384d4ec6abd63e32783ab467","IPY_MODEL_38e63bf0128f40999e476280d4f2238c"],"layout":"IPY_MODEL_d214f84632c4431299cc17c3c19601e9"}},"59a5c43fc5d94544a07ea8b64809ed10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3766a8c221b3436b9a3141f1d9a8c894","placeholder":"​","style":"IPY_MODEL_a000d9f961394afa9718e1a69272deb3","value":"Downloading (…)okenizer_config.json: 100%"}},"e9de6761384d4ec6abd63e32783ab467":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8792930cc814b0b94435f7ca7ddac55","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f66be34139149d8a6b577bd4f55e4a2","value":28}},"38e63bf0128f40999e476280d4f2238c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_521eca2a89f74c1293a80d902d231102","placeholder":"​","style":"IPY_MODEL_cb5c19e6d0064a748dd366288a67a417","value":" 28.0/28.0 [00:00&lt;00:00, 1.47kB/s]"}},"d214f84632c4431299cc17c3c19601e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3766a8c221b3436b9a3141f1d9a8c894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a000d9f961394afa9718e1a69272deb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8792930cc814b0b94435f7ca7ddac55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f66be34139149d8a6b577bd4f55e4a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"521eca2a89f74c1293a80d902d231102":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb5c19e6d0064a748dd366288a67a417":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65469a6ddfbc4ec8a7d3d3866333396a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6931930a283f4d5086781573ae5b4d0a","IPY_MODEL_accde51114e3410991246ed0d6f8c857","IPY_MODEL_51ebde9c0fff419d8a5a3247e116a2ab"],"layout":"IPY_MODEL_9bd0ab2e326b4a8d92d1ebc934c38a13"}},"6931930a283f4d5086781573ae5b4d0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b25c34c24b144999bfa475f878262543","placeholder":"​","style":"IPY_MODEL_9cafabd1d4ba4e83a0bca830087cbd96","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"accde51114e3410991246ed0d6f8c857":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb7b9ceb0ba0475792b935537fd735de","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d37cc33e1f374583a2859afd052b6113","value":231508}},"51ebde9c0fff419d8a5a3247e116a2ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b8f6d8eff5143e98536103ca6c66ba4","placeholder":"​","style":"IPY_MODEL_5c00e2776f9b41d3b17b218c091da3f4","value":" 232k/232k [00:00&lt;00:00, 13.2MB/s]"}},"9bd0ab2e326b4a8d92d1ebc934c38a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b25c34c24b144999bfa475f878262543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cafabd1d4ba4e83a0bca830087cbd96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb7b9ceb0ba0475792b935537fd735de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d37cc33e1f374583a2859afd052b6113":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b8f6d8eff5143e98536103ca6c66ba4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c00e2776f9b41d3b17b218c091da3f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d67a91dd72af4b0dadd5e98508b4146b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5302aeaa888541a4a4a051813be02265","IPY_MODEL_e1e4278fa89f4337ad0cda6fef69b09d","IPY_MODEL_f74ec8597c5a4c0c92d521041273763d"],"layout":"IPY_MODEL_ea32a6245dba4bf5a29b1f222397afd8"}},"5302aeaa888541a4a4a051813be02265":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce9e522a16a4454793d79220546d0604","placeholder":"​","style":"IPY_MODEL_fe4da26381e147c584078c61cb2fc9ee","value":"Downloading (…)/main/tokenizer.json: 100%"}},"e1e4278fa89f4337ad0cda6fef69b09d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d72e2619f5504138b433dadaa14c6a1a","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13cf17fa1cdb4bc9aa1888d42b64ea2a","value":466062}},"f74ec8597c5a4c0c92d521041273763d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f62e18e73154b69a35caddaa6f65d32","placeholder":"​","style":"IPY_MODEL_01aeac8bbcf34bc39d5c797276ce3f9d","value":" 466k/466k [00:00&lt;00:00, 1.92MB/s]"}},"ea32a6245dba4bf5a29b1f222397afd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce9e522a16a4454793d79220546d0604":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe4da26381e147c584078c61cb2fc9ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d72e2619f5504138b433dadaa14c6a1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13cf17fa1cdb4bc9aa1888d42b64ea2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f62e18e73154b69a35caddaa6f65d32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01aeac8bbcf34bc39d5c797276ce3f9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Neuron2Graph"],"metadata":{"id":"ArdL1TV31gsG"}},{"cell_type":"markdown","source":["Download `word_to_casings.json` from `https://github.com/apartresearch/Neuron2Graph/n2g/data`\n","and save to `base_path/data` (see below)\n","\n","Download the relevant `activation_matrix` file from `https://github.com/apartresearch/Neuron2Graph/n2g/data` (if present) and save `base_path/data`. Otherwi"],"metadata":{"id":"lrYQofue1pCz"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"_niVk77V1jSQ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C41oFeoe1df-","executionInfo":{"status":"ok","timestamp":1692652802884,"user_tz":-60,"elapsed":17805,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}},"outputId":"f8477c13-c2e7-4e5d-c031-546fe62b3ed1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Set `base_path` to the folder containing this notebook. Create a directory called `data` in this folder."],"metadata":{"id":"j6mQjMiG20fR"}},{"cell_type":"code","source":["import os\n","\n","# Set this appropriately\n","base_path = \"/content/drive/MyDrive/Interpretability\""],"metadata":{"id":"9xzsAEBs1yUh","executionInfo":{"status":"ok","timestamp":1692652802884,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install -q transformer_lens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRNC0XwY159W","executionInfo":{"status":"ok","timestamp":1692652838399,"user_tz":-60,"elapsed":35517,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}},"outputId":"56222fdf-1543-4b81-f586-d4074a7a02a3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m812.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","import tqdm.notebook as tqdm\n","\n","import random\n","import time\n","\n","# from google.colab import drive\n","from pathlib import Path\n","import pickle\n","import os\n","\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from torch.utils.data import DataLoader\n","\n","from functools import *\n","import pandas as pd\n","import gc\n","import collections\n","import copy\n","\n","# import comet_ml\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","\n","from pprint import pprint"],"metadata":{"id":"TupaF2BS3B4_","executionInfo":{"status":"ok","timestamp":1692652845056,"user_tz":-60,"elapsed":6662,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformer_lens.utils import (\n","    gelu_new,\n","    to_numpy,\n","    get_corner,\n","    lm_cross_entropy_loss,\n","    sample_logits\n",")  # Helper functions\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig"],"metadata":{"id":"BZXa807b3Mi1","executionInfo":{"status":"ok","timestamp":1692652846441,"user_tz":-60,"elapsed":1387,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"79N-ph_E3Nw4","executionInfo":{"status":"ok","timestamp":1692652846442,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYdgLiHo3cKa","executionInfo":{"status":"ok","timestamp":1692652848363,"user_tz":-60,"elapsed":1924,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}},"outputId":"4f3a7527-27d0-4d1a-a0c9-10afbf550b0e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Define Models"],"metadata":{"id":"4GTg7uRu3WMp"}},{"cell_type":"markdown","source":["Set the model name to the name of the model on https://neuroscope.io/index.html\n","\n","See https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=i7yfqSeOuBMg7g6qohK86DsI for Neuroscope documentation - in particular\n","\n","```\n","For SoLU models, I use the hook_mid activation - after the SoLU but before the LayerNorm\n","For GELU models I use hook_post, the activation after the GELU\n","```\n","\n","So set `layer_ending` to `\"mlp.hook_mid\"` for SoLU models and `\"mlp.hook_post\"` for GeLU models"],"metadata":{"id":"Py3yWgFM3h4C"}},{"cell_type":"code","source":["\n","from transformers import AutoModelForCausalLM\n","\n","model_name = \"solu-6l-pile\"\n","layer_ending = \"mlp.hook_mid\"\n","model = HookedTransformer.from_pretrained(model_name).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KTsBLux3SI6","executionInfo":{"status":"ok","timestamp":1692654221918,"user_tz":-60,"elapsed":3654,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}},"outputId":"9bccebcc-a885-4d4b-a674-6611eb31992f"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stderr","text":["Using pad_token, but it is not set yet.\n","WARNING:root:This model is using final RMS normalization, so the writing weights can't be centered! Skipping\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model solu-6l-pile into HookedTransformer\n","Moving model to device:  cuda:0\n"]}]},{"cell_type":"markdown","source":["If the below fails, make sure you've scraped the activations and saved them in the correct place. See the section `Scrape Activations`."],"metadata":{"id":"hPGZMOpT6r_-"}},{"cell_type":"code","source":["with open(os.path.join(base_path, f\"data/activation_matrix-{model_name}.json\")) as ifh:\n","    activation_matrix = json.load(ifh)\n","    activation_matrix = np.array(activation_matrix)"],"metadata":{"id":"9RLo-FLb5ax9","executionInfo":{"status":"ok","timestamp":1692654225839,"user_tz":-60,"elapsed":1085,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForMaskedLM\n","from transformers import AutoTokenizer\n","\n","aug_model_checkpoint = \"distilbert-base-uncased\"\n","aug_model = AutoModelForMaskedLM.from_pretrained(aug_model_checkpoint).to(device)\n","aug_tokenizer = AutoTokenizer.from_pretrained(aug_model_checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["4b6e3059a2d1419d959f15e8b25cc83c","c6aa4839f72e40efa568688bea920048","7ad7e3b4abf74d3da4d85d713cf689bd","7ee80cbeda3c4f8693dca4bd8ee3b094","500bd098f31c41d2a8e22d47ab3a4722","dc5507da728a456db8da86548b36dc98","43e9960b13ea4e5fb9e55e7e29bdb3c3","49c2ce1615f04fd78a11c7b00c38d809","f4165d6a45d845ffa133c73075542228","891f48d84fc946e7919d38699f0992b7","23ef0d518f4d48b3a40df32db3785c19","5ff6b8142d2f409fb1b7576a11d08f61","6616deab6ee74b64b04a2a779098130e","640c02e25d5e4ef0aa5ecbe7f2835f11","5fdf00eb7f31407fbb096a223cdd8aeb","eb87ac0a072647d18f7cb8b2b5ee06eb","7faf98a913cb41e08e1fe978b650cc20","fa1f018adbde4b09bfe5c662c6552c98","eb07d48ceb3d4fae9e176639f6e534b3","cd9a8e65e8334ebc9e4b01ad707dc5bc","ba24f1a604a14fbab0a2706ac257a5b6","6fc636d9a86641a88ebd6697cf1b2177","55b27838da544005ba26ae95a8eb5ca4","59a5c43fc5d94544a07ea8b64809ed10","e9de6761384d4ec6abd63e32783ab467","38e63bf0128f40999e476280d4f2238c","d214f84632c4431299cc17c3c19601e9","3766a8c221b3436b9a3141f1d9a8c894","a000d9f961394afa9718e1a69272deb3","c8792930cc814b0b94435f7ca7ddac55","4f66be34139149d8a6b577bd4f55e4a2","521eca2a89f74c1293a80d902d231102","cb5c19e6d0064a748dd366288a67a417","65469a6ddfbc4ec8a7d3d3866333396a","6931930a283f4d5086781573ae5b4d0a","accde51114e3410991246ed0d6f8c857","51ebde9c0fff419d8a5a3247e116a2ab","9bd0ab2e326b4a8d92d1ebc934c38a13","b25c34c24b144999bfa475f878262543","9cafabd1d4ba4e83a0bca830087cbd96","cb7b9ceb0ba0475792b935537fd735de","d37cc33e1f374583a2859afd052b6113","6b8f6d8eff5143e98536103ca6c66ba4","5c00e2776f9b41d3b17b218c091da3f4","d67a91dd72af4b0dadd5e98508b4146b","5302aeaa888541a4a4a051813be02265","e1e4278fa89f4337ad0cda6fef69b09d","f74ec8597c5a4c0c92d521041273763d","ea32a6245dba4bf5a29b1f222397afd8","ce9e522a16a4454793d79220546d0604","fe4da26381e147c584078c61cb2fc9ee","d72e2619f5504138b433dadaa14c6a1a","13cf17fa1cdb4bc9aa1888d42b64ea2a","2f62e18e73154b69a35caddaa6f65d32","01aeac8bbcf34bc39d5c797276ce3f9d"]},"id":"H6SOHMZd3fwX","executionInfo":{"status":"ok","timestamp":1692652912976,"user_tz":-60,"elapsed":5683,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}},"outputId":"540b9c54-e699-4f2c-9dbd-9dc3624c39d8"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6e3059a2d1419d959f15e8b25cc83c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff6b8142d2f409fb1b7576a11d08f61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b27838da544005ba26ae95a8eb5ca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65469a6ddfbc4ec8a7d3d3866333396a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67a91dd72af4b0dadd5e98508b4146b"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Scrape Activations"],"metadata":{"id":"LYSQPg-c54RK"}},{"cell_type":"markdown","source":["If you're running a model for the first time, you have to scrape the max activation for each neuron from Neuroscope.\n","\n","Set layers, neurons, and model_name to the appropriate values (see https://neuroscope.io/index.html) and run the cells"],"metadata":{"id":"tBlYvL3B58Ka"}},{"cell_type":"code","source":["def get_max_acts(model_name, layer_and_neurons):\n","  layer, neurons = layer_and_neurons\n","  activations = []\n","  for i, neuron in enumerate(neurons):\n","    if i % 50 == 0:\n","      print(f\"\\nLayer {layer}: {i} of {len(neurons)} complete\")\n","    try:\n","      activation = get_max_activations(model_name, layer, neuron, n=1)\n","      activations.append(activation)\n","    except:\n","      print(f\"Neuron {neuron} in layer {layer} failed\")\n","      # Use the previous activation as a hack to get around failures\n","      activations.append(activations[-1])\n","  return activations"],"metadata":{"id":"96HEzkmF6H59","executionInfo":{"status":"ok","timestamp":1692652912976,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["%%time\n","import json\n","import multiprocessing as mp\n","\n","run_scraping = False\n","\n","layers = 24\n","neurons = 4096\n","\n","if run_scraping:\n","  info = [(layer, [neuron for neuron in range(neurons)]) for layer in range(layers)]\n","\n","  with mp.Pool(layers) as p:\n","    activation_matrix = p.map(partial(get_max_acts, model_name), info)\n","\n","  activation_matrix_np = np.array(activation_matrix)\n","\n","  with open(os.path.join(base_path, f\"data/activation_matrix-{model_name}.json\"), \"w\") as ofh:\n","    json.dump(activation_matrix, ofh, indent=2, ensure_ascii=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUDGERQy-EKA","executionInfo":{"status":"ok","timestamp":1692654168083,"user_tz":-60,"elapsed":612,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}},"outputId":"e85f42a1-d40b-4279-b1cf-293d14b5f8fd"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 7 µs, sys: 2 µs, total: 9 µs\n","Wall time: 14.8 µs\n"]}]},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"kAKHcu6r4K1O"}},{"cell_type":"markdown","source":["### Utils"],"metadata":{"id":"8zA61mLl4RL9"}},{"cell_type":"code","source":["import requests\n","import re\n","import json\n","\n","\n","parser = re.compile('\\{\\\"tokens\\\": ')\n","def get_snippets(model_name, layer, neuron):\n","  \"\"\"Get the max activating dataset examples for a given neuron in a model\"\"\"\n","  base_url = f\"https://neuroscope.io/{model_name}/{layer}/{neuron}.html\"\n","\n","  response = requests.get(base_url)\n","  webpage = response.text\n","\n","  parts = parser.split(webpage)\n","  snippets = []\n","  for i, part in enumerate(parts):\n","    if i == 0 or i % 2 != 0:\n","      continue\n","\n","    token_str = part.split(', \"values\": ')[0]\n","\n","    tokens = json.loads(token_str)\n","\n","    snippet = \"\".join(tokens)\n","\n","    snippets.append(snippet)\n","\n","  if len(snippets) != 20:\n","    raise Exception\n","  return snippets"],"metadata":{"id":"fHHl5KdU4MEf","executionInfo":{"status":"ok","timestamp":1692652977393,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["act_parser = re.compile('<h4>Max Act: <b>')\n","def get_max_activations(model_name, layer, neuron, n=1):\n","  \"\"\"Get the max activating dataset examples for a given neuron in a model\"\"\"\n","  base_url = f\"https://neuroscope.io/{model_name}/{layer}/{neuron}.html\"\n","\n","  response = requests.get(base_url)\n","  webpage = response.text\n","\n","  parts = act_parser.split(webpage)\n","  activations = []\n","  for i, part in enumerate(parts):\n","    if i == 0:\n","      continue\n","\n","    activation = float(part.split('</b>')[0])\n","\n","    activations.append(activation)\n","    if len(activations) >= n:\n","      break\n","\n","  if len(activations) != min(20, n):\n","    raise Exception\n","  return activations if n > 1 else activations[0]"],"metadata":{"id":"F8ts_lWi4Tsb","executionInfo":{"status":"ok","timestamp":1692652977393,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["from string import punctuation\n","\n","class WordTokenizer:\n","  \"\"\"Simple tokenizer for splitting text into words\"\"\"\n","\n","  def __init__(self, split_tokens, stick_tokens):\n","    self.split_tokens = split_tokens\n","    self.stick_tokens = stick_tokens\n","\n","  def __call__(self, text):\n","    return self.tokenize(text)\n","\n","  def is_split(self, char):\n","    \"\"\"Split on any non-alphabet chars unless excluded, and split on any specified chars\"\"\"\n","    return char in self.split_tokens or (not char.isalpha() and char not in self.stick_tokens)\n","\n","  def tokenize(self, text):\n","    \"\"\"Tokenize text, preserving all characters\"\"\"\n","    tokens = []\n","    current_token = \"\"\n","    for char in text:\n","      if self.is_split(char):\n","        tokens.append(current_token)\n","        tokens.append(char)\n","        current_token = \"\"\n","        continue\n","      current_token += char\n","    tokens.append(current_token)\n","    tokens = [token for token in tokens if token]\n","    return tokens\n","\n","stick_tokens = {\"'\"}\n","word_tokenizer = WordTokenizer(set(), stick_tokens)"],"metadata":{"id":"9QyS1uUN4UYZ","executionInfo":{"status":"ok","timestamp":1692652977394,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["with open(f\"{base_path}/data/word_to_casings.json\") as ifh:\n","  word_to_casings = json.load(ifh)"],"metadata":{"id":"nxjho9HX4X4f","executionInfo":{"status":"ok","timestamp":1692652982504,"user_tz":-60,"elapsed":5114,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def layer_index_to_name(layer_index):\n","  return f\"blocks.{layer_index}.{layer_ending}\""],"metadata":{"id":"CLtHKl304wEV","executionInfo":{"status":"ok","timestamp":1692652982504,"user_tz":-60,"elapsed":6,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from pprint import pprint\n","from collections import defaultdict\n","from string import punctuation\n","import re\n","import copy\n","\n","\n","splitter = re.compile(\"[\\.!\\\\n]\")\n","\n","def sentence_tokenizer(str_tokens):\n","  \"\"\"Split tokenized text into sentences\"\"\"\n","  sentences = []\n","  sentence = []\n","  sentence_to_token_indices = defaultdict(list)\n","  token_to_sentence_indices = {}\n","\n","  for i, str_token in enumerate(str_tokens):\n","    sentence.append(str_token)\n","    sentence_to_token_indices[len(sentences)].append(i)\n","    token_to_sentence_indices[i] = len(sentences)\n","    if splitter.search(str_token) is not None or i + 1 == len(str_tokens):\n","      sentences.append(sentence)\n","      sentence = []\n","\n","  return sentences, sentence_to_token_indices, token_to_sentence_indices"],"metadata":{"id":"0x2mgWWw4ylh","executionInfo":{"status":"ok","timestamp":1692652982504,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["import math\n","\n","\n","def batch(arr, n=None, batch_size=None):\n","    if n is None and batch_size is None:\n","        raise ValueError(\"Either n or batch_size must be provided\")\n","    if n is not None and batch_size is not None:\n","        raise ValueError(\"Either n or batch_size must be provided, not both\")\n","\n","    if n is not None:\n","        batch_size = math.floor(len(arr) / n)\n","    elif batch_size is not None:\n","        n = math.ceil(len(arr) / batch_size)\n","\n","    extras = len(arr) - (batch_size * n)\n","    groups = []\n","    group = []\n","    added_extra = False\n","    for element in arr:\n","        group.append(element)\n","        if len(group) >= batch_size:\n","            if extras and not added_extra:\n","                extras -= 1\n","                added_extra = True\n","                continue\n","            groups.append(group)\n","            group = []\n","            added_extra = False\n","\n","    if group:\n","        groups.append(group)\n","\n","    return groups"],"metadata":{"id":"5rbMxOjY4_Fj","executionInfo":{"status":"ok","timestamp":1692652982505,"user_tz":-60,"elapsed":6,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["### Augment"],"metadata":{"id":"S2JHPnMM4kbz"}},{"cell_type":"code","source":["from ast import Continue\n","import copy\n","from nltk.corpus import stopwords\n","from string import punctuation\n","import re\n","from scipy.special import softmax\n","\n","class FastAugmenter:\n","  \"\"\"Uses BERT to generate variations on input text by masking words and substituting with most likely predictions\"\"\"\n","\n","  def __init__(self, model, model_tokenizer, word_tokenizer, neuron_model, device=\"cuda:0\"):\n","    self.model = model\n","    self.model_tokenizer = model_tokenizer\n","    self.stops = set(stopwords.words('english'))\n","    self.punctuation_set = set(punctuation)\n","    self.to_strip = \" \" + punctuation\n","    self.word_tokenizer = word_tokenizer\n","    self.device = device\n","\n","  def augment(self, text, max_char_position=None, exclude_stopwords=False, n=5, important_tokens=None, **kwargs):\n","    joiner = \"\"\n","    tokens = self.word_tokenizer(text)\n","\n","    new_texts = []\n","    positions = []\n","\n","    important_tokens = {token.strip(self.to_strip).lower() for token in important_tokens}\n","\n","    seen_prompts = set()\n","\n","    # Gather all tokens to be substituted\n","    tokens_to_sub = []\n","\n","    # Mask important tokens\n","    masked_token_sets = []\n","    masked_texts = []\n","\n","    masked_tokens = []\n","\n","    for i, token in enumerate(tokens):\n","      norm_token = token.strip(self.to_strip).lower() if any(c.isalpha() for c in token) else token\n","\n","      if not token or word_tokenizer.is_split(token) or (exclude_stopwords and norm_token in self.stops) or (important_tokens is not None and norm_token not in important_tokens):\n","        continue\n","\n","      # If no alphanumeric characters, we'll do a special substitution rather than using BERT\n","      if not any(c.isalpha() for c in token):\n","        continue\n","\n","      before = tokens[:i]\n","      before_text = joiner.join(before)\n","      position = len(before_text)\n","\n","      # Don't bother if we're beyond the max activating token, as these tokens have no effect on the activation\n","      if max_char_position is not None and position > max_char_position:\n","        break\n","\n","      copy_tokens = copy.deepcopy(tokens)\n","      copy_tokens[i] = \"[MASK]\"\n","      masked_token_sets.append((copy_tokens, position))\n","      masked_texts.append(joiner.join(copy_tokens))\n","\n","      masked_tokens.append(token)\n","\n","    # pprint(masked_texts)\n","    if len(masked_texts) == 0:\n","      return [], []\n","\n","    inputs = self.model_tokenizer(masked_texts, padding=True, return_tensors=\"pt\").to(self.device)\n","    token_probs = softmax(self.model(**inputs).logits.cpu().detach().numpy(), axis=-1)\n","    inputs = inputs.to(\"cpu\")\n","\n","    chosen_tokens = set()\n","\n","    new_texts = []\n","    positions = []\n","\n","    seen_texts = set()\n","\n","    for i, (masked_token_set, char_position) in enumerate(masked_token_sets):\n","      mask_token_index = np.argwhere(inputs[\"input_ids\"][i] == self.model_tokenizer.mask_token_id)[0, 0]\n","\n","      mask_token_probs = token_probs[i, mask_token_index, :]\n","\n","      # We negate the array before argsort to get the largest, not the smallest, logits\n","      top_probs = -np.sort(-mask_token_probs).transpose()\n","      top_tokens = np.argsort(-mask_token_probs).transpose()\n","\n","      subbed = 0\n","\n","      # Substitute the given token with the best predictions\n","      for l, (top_token, top_prob) in enumerate(zip(top_tokens, top_probs)):\n","        if top_prob < 0.00001:\n","          break\n","\n","        candidate_token = self.model_tokenizer.decode(top_token)\n","\n","        # print(candidate_token)\n","\n","        # Check that the predicted token isn't the same as the token that was already there\n","        normalised_candidate = candidate_token.strip(self.to_strip).lower() if candidate_token not in self.punctuation_set else candidate_token\n","        normalised_token = token.strip(self.to_strip).lower() if token not in self.punctuation_set else token\n","\n","        if normalised_candidate == normalised_token or not any(c.isalpha() for c in candidate_token):\n","          continue\n","\n","        # Get most common casing of the word\n","        most_common_casing = word_to_casings.get(candidate_token, [(candidate_token, 1)])[0][0]\n","\n","        original_token = masked_tokens[i]\n","        # Title case normally has meaning (e.g., start of sentence, in a proper noun, etc.) so follow original token, otherwise use most common\n","        best_casing = candidate_token.title() if original_token.istitle() else most_common_casing\n","\n","        new_token_set = copy.deepcopy(masked_token_set)\n","        # BERT uses ## to denote a tokenisation within a word, so we remove it to glue the word back together\n","        masked_text = joiner.join(new_token_set)\n","        new_text = masked_text.replace(self.model_tokenizer.mask_token, best_casing, 1).replace(\" ##\", \"\")\n","\n","        if new_text in seen_texts:\n","          continue\n","\n","        new_texts.append(new_text)\n","        positions.append(char_position)\n","        subbed += 1\n","\n","        if subbed >= n:\n","          break\n","\n","    return new_texts, positions"],"metadata":{"id":"0TamVkof4lwt","executionInfo":{"status":"ok","timestamp":1692652982505,"user_tz":-60,"elapsed":6,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["fast_aug = FastAugmenter(aug_model, aug_tokenizer, word_tokenizer, model)"],"metadata":{"id":"LQ1M7DYa4pHt","executionInfo":{"status":"ok","timestamp":1692652982505,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def augment(model, layer, index, prompt, aug, max_length=1024, inclusion_threshold=-0.5, exclusion_threshold=-0.5, n=5, **kwargs):\n","  \"\"\"Generate variations of a prompt using an augmenter\"\"\"\n","  prepend_bos = True\n","  tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n","  str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n","\n","  # print(prompt)\n","\n","  if len(tokens[0]) > max_length:\n","    tokens = tokens[0, :max_length].unsqueeze(0)\n","\n","  logits, cache = model.run_with_cache(tokens)\n","  activations = cache[layer][0, :, index]\n","\n","  initial_max = torch.max(activations).cpu().item()\n","  initial_argmax = torch.argmax(activations).cpu().item()\n","  max_char_position = len(\"\".join(str_tokens[int(prepend_bos):initial_argmax + 1]))\n","\n","  positive_prompts = [(prompt, initial_max, 1)]\n","  negative_prompts = []\n","\n","  if n == 0:\n","    return positive_prompts, negative_prompts\n","\n","  aug_prompts, aug_positions = aug.augment(prompt, max_char_position=max_char_position, n=n, **kwargs)\n","  if not aug_prompts:\n","    return positive_prompts, negative_prompts\n","\n","  aug_tokens = model.to_tokens(aug_prompts, prepend_bos=prepend_bos)\n","\n","  aug_logits, aug_cache = model.run_with_cache(aug_tokens)\n","  all_aug_activations = aug_cache[layer][:, :, index]\n","\n","  for aug_prompt, char_position, aug_activations in zip(aug_prompts, aug_positions, all_aug_activations):\n","    aug_max = torch.max(aug_activations).cpu().item()\n","    aug_argmax = torch.argmax(aug_activations).cpu().item()\n","\n","    # TODO implement this properly - when we mask multiple tokens, if they cross the max_char_position this will not necessarily be correct\n","    if char_position < max_char_position:\n","      new_str_tokens = model.to_str_tokens(aug_prompt, prepend_bos=prepend_bos)\n","      aug_argmax += len(new_str_tokens) - len(str_tokens)\n","\n","    proportion_drop = (aug_max - initial_max) / initial_max\n","\n","    if proportion_drop >= inclusion_threshold:\n","      positive_prompts.append((aug_prompt, aug_max, proportion_drop))\n","    elif proportion_drop < exclusion_threshold:\n","      negative_prompts.append((aug_prompt, aug_max, proportion_drop))\n","\n","  return positive_prompts, negative_prompts"],"metadata":{"id":"zCFv1pe148sb","executionInfo":{"status":"ok","timestamp":1692652982506,"user_tz":-60,"elapsed":6,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["### Prune"],"metadata":{"id":"SMN4srlP448Z"}},{"cell_type":"code","source":["def fast_prune(model, layer, neuron, prompt, max_length=1024, proportion_threshold=-0.5, absolute_threshold=None, token_activation_threshold=0.75, window=0, return_maxes=False, cutoff=30, batch_size=4, max_post_context_tokens=5, skip_threshold=0, skip_interval=5, return_intermediates=False, **kwargs):\n","  \"\"\"Prune an input prompt to the shortest string that preserves x% of neuron activation on the most activating token.\"\"\"\n","\n","  prepend_bos = True\n","  tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n","  str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n","\n","  if len(tokens[0]) > max_length:\n","    tokens = tokens[0, :max_length].unsqueeze(0)\n","\n","  logits, cache = model.run_with_cache(tokens)\n","  logits = logits.cpu()\n","  activations = cache[layer][0, :, neuron].cpu()\n","\n","  full_initial_max = torch.max(activations).cpu().item()\n","  full_initial_argmax = torch.argmax(activations).cpu().item()\n","\n","  sentences, sentence_to_token_indices, token_to_sentence_indices = sentence_tokenizer(str_tokens)\n","\n","  # print(activation_threshold * full_initial_max)\n","\n","  strong_indices = torch.where(activations >= token_activation_threshold * full_initial_max)[0]\n","  strong_activations = activations[strong_indices].cpu()\n","  strong_indices = strong_indices.cpu()\n","\n","  # print(strong_activations)\n","  # print(strong_indices)\n","\n","  strong_sentence_indices = [token_to_sentence_indices[index.item()] for index in strong_indices]\n","\n","  pruned_sentences = []\n","  final_max_indices = []\n","  all_intermediates = []\n","  initial_maxes = []\n","  truncated_maxes = []\n","\n","  for strong_sentence_index, initial_argmax, initial_max in zip(strong_sentence_indices, strong_indices, strong_activations):\n","    initial_argmax = initial_argmax.item()\n","    initial_max = initial_max.item()\n","    # print(strong_sentence_index, initial_argmax, initial_max)\n","\n","    max_sentence_index = token_to_sentence_indices[initial_argmax]\n","    relevant_str_tokens = [str_token for sentence in sentences[:max_sentence_index + 1] for str_token in sentence]\n","\n","    prior_context = relevant_str_tokens[:initial_argmax + 1]\n","\n","    post_context = relevant_str_tokens[initial_argmax + 1:]\n","\n","    shortest_successful_prompt = None\n","    final_max_index = None\n","\n","    truncated_prompts = []\n","    added_tokens = []\n","\n","    count = 0\n","    full_prior = prior_context[:max(0, initial_argmax - window + 1)]\n","\n","    for i, str_token in reversed(list(enumerate(full_prior))):\n","      count += 1\n","\n","      if count > cutoff:\n","        break\n","\n","      # print(count, len(full_prior))\n","\n","      if not count == len(full_prior) and count >= skip_threshold and count % skip_interval != 0:\n","        continue\n","\n","      # print(\"Made it!\")\n","\n","      truncated_prompt = prior_context[i:]\n","      joined = \"\".join(truncated_prompt)\n","      truncated_prompts.append(joined)\n","      added_tokens.append(i)\n","\n","    batched_truncated_prompts = batch(truncated_prompts, batch_size=batch_size)\n","    batched_added_tokens = batch(added_tokens, batch_size=batch_size)\n","\n","    finished = False\n","    intermediates = []\n","    for i, (truncated_batch, added_tokens_batch) in enumerate(zip(batched_truncated_prompts, batched_added_tokens)):\n","      # print(\"length\", len(truncated_batch))\n","      # pprint(truncated_batch)\n","\n","      truncated_tokens = model.to_tokens(truncated_batch, prepend_bos=prepend_bos)\n","\n","      # pprint(truncated_tokens)\n","\n","      logits, cache = model.run_with_cache(truncated_tokens)\n","      all_truncated_activations = cache[layer][:, :, neuron].cpu()\n","\n","      # print(\"shape\", all_truncated_activations.shape)\n","\n","      for j, truncated_activations in enumerate(all_truncated_activations):\n","        num_added_tokens = added_tokens_batch[j]\n","        # print(\"single shape\", truncated_activations.shape)\n","        truncated_argmax = torch.argmax(truncated_activations).cpu().item() + num_added_tokens\n","        final_max_index = torch.argmax(truncated_activations).cpu().item()\n","\n","        if prepend_bos:\n","          truncated_argmax -= 1\n","          final_max_index -= 1\n","        truncated_max = torch.max(truncated_activations).cpu().item()\n","\n","        # trunc_logits, trunc_cache = model.run_with_cache(model.to_tokens(truncated_batch[j], prepend_bos=prepend_bos))\n","        # trunc_activations = trunc_cache[layer][0, :, neuron]\n","\n","        # print(truncated_activations)\n","        # print(trunc_activations)\n","        # print(\"truncated_argmax\", truncated_argmax)\n","        # print(truncated_max)\n","\n","        shortest_prompt = truncated_batch[j]\n","\n","        if not shortest_prompt.startswith(\"<|endoftext|>\"):\n","          truncated_str_tokens = model.to_str_tokens(truncated_batch[j], prepend_bos=False)\n","          intermediates.append((shortest_prompt, truncated_str_tokens[0], truncated_max))\n","\n","        if (truncated_argmax == initial_argmax and (\n","            (truncated_max - initial_max) / initial_max > proportion_threshold or\n","            (absolute_threshold is not None and truncated_max >= absolute_threshold))) or (i == len(batched_truncated_prompts) - 1 and j == len(all_truncated_activations) - 1):\n","          shortest_successful_prompt = shortest_prompt\n","          finished = True\n","          break\n","\n","      if finished:\n","        break\n","\n","    # if shortest_successful_prompt is None:\n","    #   pruned_sentence = \"\".join(relevant_str_tokens)\n","    #   final_max_index = initial_argmax\n","    # else:\n","    pruned_sentence = \"\".join(shortest_successful_prompt) # if shortest_successful_prompt is not None else shortest_prompt\n","\n","    if max_post_context_tokens is not None:\n","      pruned_sentence += \"\".join(post_context[:max_post_context_tokens])\n","\n","    pruned_sentences.append(pruned_sentence)\n","    final_max_indices.append(final_max_index)\n","    initial_maxes.append(initial_max)\n","    truncated_maxes.append(truncated_max)\n","    all_intermediates.append(intermediates)\n","\n","  if return_maxes:\n","    return list(zip(pruned_sentences, final_max_indices, initial_maxes, truncated_maxes))\n","\n","  elif return_intermediates:\n","    return list(zip(pruned_sentences, all_intermediates))\n","\n","  return list(zip(pruned_sentences, final_max_indices))"],"metadata":{"id":"0OmvlbAI4q4O","executionInfo":{"status":"ok","timestamp":1692652982506,"user_tz":-60,"elapsed":6,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["### Measure Importance"],"metadata":{"id":"S8-fqvbA5MW-"}},{"cell_type":"code","source":["import numpy as np\n","import copy\n","\n","def fast_measure_importance(model, layer, neuron, prompt, initial_argmax=None, max_length=1024, max_activation=None, masking_token=1, threshold=0.8, scale_factor=1, return_all=False, activation_threshold=0.1, **kwargs):\n","  \"\"\"Compute a measure of token importance by masking each token and measuring the drop in activation on the max activating token\"\"\"\n","\n","  prepend_bos = True\n","  tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n","  str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n","\n","  if len(tokens[0]) > max_length:\n","    tokens = tokens[0, :max_length].unsqueeze(0)\n","\n","  # logits, cache = model.run_with_cache(tokens)\n","\n","  # print(tokens_and_activations)\n","\n","  importances_matrix = []\n","\n","  shortest_successful_prompt = None\n","  # cutoff = 50\n","\n","  masked_prompts = tokens.repeat(len(tokens[0]) + 1, 1)\n","\n","  # print(f\"{len(masked_prompts)=}, {initial_argmax=}, {starting_point=}\")\n","\n","  for i in range(1, len(masked_prompts)):\n","    masked_prompts[i, i - 1] = masking_token\n","\n","  # for i, str_token in enumerate(str_tokens):\n","  #   if i >= cutoff:\n","  #     break\n","\n","  #   masked_tokens = tokens\n","\n","  #   if i >= len(masked_tokens[0]):\n","  #     continue\n","\n","  #   token_to_mask = copy.deepcopy(tokens[0, i])\n","  #   masked_tokens[0, i] = masking_token\n","\n","  #   masked_prompts.append(masked_tokens[0])\n","  #   tokens[0, i] = token_to_mask\n","\n","  # pprint(masked_prompts)\n","\n","  logits, cache = model.run_with_cache(masked_prompts)\n","  logits = logits.cpu()\n","  all_masked_activations = cache[layer][1:, :, neuron].cpu()\n","\n","  activations = cache[layer][0, :, neuron].cpu()\n","\n","  if initial_argmax is None:\n","    initial_argmax = torch.argmax(activations).cpu().item()\n","  else:\n","    # This could be wrong\n","    initial_argmax = min(initial_argmax, len(activations) - 1)\n","\n","  # print(activations)\n","  # print(activation_threshold)\n","  # activation_indexes = [i for i, activation in enumerate(activations) if activation * scale_factor / max_activation > activation_threshold]\n","  # print(activation_indexes)\n","  # final_activating = initial_argmax if len(activation_indexes) == 0 else activation_indexes[-1]\n","\n","  initial_max = activations[initial_argmax].cpu().item()\n","\n","  if max_activation is None:\n","    max_activation = initial_max\n","  scale = min(1, initial_max / max_activation)\n","\n","  # print(\"scale_factor measure_importance\", scale_factor)\n","\n","  tokens_and_activations = [[str_token, round(activation.cpu().item() * scale_factor / max_activation, 3)] for str_token, activation in zip(str_tokens, activations)]\n","  important_tokens = []\n","  tokens_and_importances = [[str_token, 0] for str_token in str_tokens]\n","\n","  for i, masked_activations in enumerate(all_masked_activations):\n","    if return_all:\n","      # Get importance of the given token for all tokens\n","      importances_row = []\n","      for j, activation in enumerate(masked_activations):\n","        activation = activation.cpu().item()\n","        normalised_activation = (1 - (activation / activations[j].cpu().item()))\n","        importances_row.append((str_tokens[j], normalised_activation))\n","\n","      # for j, str_token in enumerate(str_tokens[cutoff:]):\n","      #   importances_row.append((str_token, 0))\n","\n","      # print(\"importances_row\", importances_row)\n","      importances_matrix.append(np.array(importances_row))\n","\n","    masked_max = masked_activations[initial_argmax].cpu().item()\n","    normalised_activation = (1 - (masked_max / initial_max))\n","\n","    str_token = tokens_and_importances[i][0]\n","    tokens_and_importances[i][1] = normalised_activation\n","    if normalised_activation >= threshold and str_token != \"<|endoftext|>\":\n","      important_tokens.append(str_token)\n","\n","  # for i, str_token in enumerate(str_tokens[cutoff:]):\n","  #   tokens_and_importances.append((str_token, 0))\n","\n","  if return_all:\n","    # Flip so we have the importance of all tokens for a given token\n","    importances_matrix = np.array(importances_matrix)\n","    return importances_matrix, initial_max, important_tokens, tokens_and_activations, initial_argmax\n","\n","  return tokens_and_importances, initial_max, important_tokens, tokens_and_activations, initial_argmax"],"metadata":{"id":"gB0Ow-Pr5KqC","executionInfo":{"status":"ok","timestamp":1692652982506,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["### Visualise"],"metadata":{"id":"8-OD2k8T5EMt"}},{"cell_type":"code","source":["import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","def visualise(tokens_and_activations, tokens_and_importances, max_index=None, title=None, truncate=False, labels=[\"Activation\", \"Importance\"], **kwargs):\n","  \"\"\"Visualise relative token activation and importance\"\"\"\n","  if max_index is None:\n","    max_index = len(tokens_and_activations)\n","\n","  zero_width = u'\\u200b'\n","  token_counter = Counter()\n","  data = {}\n","  count = 0\n","\n","  for i, ((token, importance), (_, activation)) in enumerate(zip(tokens_and_importances, tokens_and_activations)):\n","    if token == \"<|endoftext|>\":\n","      continue\n","\n","    if i > max_index and truncate:\n","      break\n","\n","    # This is a horrible hack to allow us to have a dict with the \"same\" token as multiple keys - by adding zero width spaces the tokens look the same but are actually different\n","    seen_count = token_counter[token]\n","    add = zero_width * seen_count\n","    deduped_token = token + add\n","    # Have to escape dollars so matplotlib doesn't interpret them as latex\n","    deduped_token = deduped_token.replace(\"$\", \"\\$\")\n","    data[deduped_token] = [activation, importance]\n","    token_counter[token] += 1\n","    count += 1\n","\n","  df = pd.DataFrame(data, index=labels)\n","  plt.figure(figsize=[int(count * 1.5), 1.2])\n","  sns.heatmap(df, vmin=0, vmax=1, xticklabels=True, annot=True)\n","\n","  if title is not None:\n","    title = title.replace(\"$\", \"\\$\")\n","    plt.title(title)"],"metadata":{"id":"Z-yzSZai5DYh","executionInfo":{"status":"ok","timestamp":1692652982913,"user_tz":-60,"elapsed":412,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"7f7DPDzt5HaN"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","\n","def train_and_eval(model, layer, neuron, aug=fast_aug, train_proportion=0.5, max_train_size=10, max_eval_size=20, fire_threshold=0.5, random_state=0, train_indexes=None, return_paths=False, **kwargs):\n","  if isinstance(layer, int):\n","    layer = layer_index_to_name(layer)\n","\n","  layer_num = int(layer.split(\".\")[1])\n","  base_max_act = float(activation_matrix[layer_num, neuron])\n","\n","  snippets = get_snippets(model_name, layer_num, neuron)\n","\n","  if train_indexes is None:\n","    train_snippets, test_snippets = train_test_split(snippets, train_size=train_proportion, random_state=random_state)\n","  else:\n","    train_snippets = [snippet for i, snippet in enumerate(snippets) if i in train_indexes]\n","    test_snippets = [snippet for i, snippet in enumerate(snippets) if i not in train_indexes]\n","  # train_data, test_data = train_test_split(data, train_size=train_proportion, random_state=0)\n","\n","  # train_data_snippets = [\"\".join(tokens) for tokens, activations in train_data if any(activation > fire_threshold for activation in activations)][:max_train_size]\n","  train_data_snippets = []\n","  all_train_snippets = train_snippets + train_data_snippets\n","\n","  all_info = []\n","  for i, snippet in enumerate(all_train_snippets):\n","    # if i % 10 == 0:\n","    print(f\"Processing {i + 1} of {len(all_train_snippets)}\")\n","\n","    pruned_results = fast_prune(model, layer, neuron, snippet, return_maxes=True, **kwargs)\n","\n","    for pruned_prompt, _, initial_max_act, truncated_max_act in pruned_results:\n","      # tokens = model.to_tokens(pruned_prompt, prepend_bos=True)\n","      # str_tokens = model.to_str_tokens(pruned_prompt, prepend_bos=True)\n","      # logits, cache = model.run_with_cache(tokens)\n","      # activations = cache[layer][0, :, neuron].cpu()\n","      # max_pruned_activation = torch.max(activations).item()\n","      scale_factor = initial_max_act / truncated_max_act\n","      # scale_factor = 1\n","\n","      # print(scale_factor)\n","      # scaled_activations = activations * scale_factor / base_max_act\n","\n","      # print(list(zip(str_tokens, activations)))\n","\n","      # print(pruned_prompt)\n","\n","      # print(len(pruned_prompt))\n","\n","      if pruned_prompt is None:\n","        continue\n","\n","      info = augment_and_return(model, layer, neuron, aug, pruned_prompt, base_max_act=base_max_act, scale_factor=scale_factor, **kwargs)\n","      all_info.append(info)\n","\n","  neuron_model = NeuronModel(layer_num, neuron, **kwargs)\n","  paths = neuron_model.fit(all_info)\n","\n","  print(\"Fitted model\")\n","\n","  max_test_data = []\n","  for snippet in test_snippets:\n","    # pruned_prompt, _ = prune(model, layer, neuron, snippet, **kwargs)\n","    # if pruned_prompt is None:\n","    #   continue\n","    tokens = model.to_tokens(snippet, prepend_bos=True)\n","    str_tokens = model.to_str_tokens(snippet, prepend_bos=True)\n","    logits, cache = model.run_with_cache(tokens)\n","    activations = cache[layer][0, :, neuron].cpu()\n","    max_test_data.append((str_tokens, activations / base_max_act))\n","\n","  # pprint(max_test_data[0])\n","  # print(\"\\n\\n\")\n","  # pprint(test_data[0])\n","\n","  # print(\"Evaluation data\")\n","  # test_data = test_data[:max_eval_size]\n","  # evaluate(neuron_model, test_data, fire_threshold=fire_threshold, **kwargs)\n","\n","  print(\"Max Activating Evaluation Data\")\n","  try:\n","    stats = evaluate(neuron_model, max_test_data, fire_threshold=fire_threshold, **kwargs)\n","  except Exception as e:\n","    stats = {}\n","    print(f\"Stats failed with error: {e}\")\n","\n","  if return_paths:\n","    return stats, paths\n","  return stats\n","\n","\n","def augment_and_return(model, layer, neuron, aug, pruned_prompt, base_max_act=None, use_index=False, scale_factor=1, **kwargs):\n","  info = []\n","  importances_matrix, initial_max_act, important_tokens, tokens_and_activations, initial_max_index = fast_measure_importance(model, layer, neuron, pruned_prompt, max_activation=base_max_act, scale_factor=scale_factor, return_all=True)\n","\n","  if base_max_act is not None:\n","    initial_max_act = base_max_act\n","\n","  positive_prompts, negative_prompts = augment(model, layer, neuron, pruned_prompt, aug, important_tokens=set(important_tokens), **kwargs)\n","\n","  for i, (prompt, activation, change) in enumerate(positive_prompts):\n","    title = prompt\n","    if i == 0:\n","      title = \"Original - \" + prompt\n","\n","    #   print(\"Original\")\n","    #   print(prompt, \"\\n\")\n","    # elif i > 1:\n","    #   print(\"Augmented\")\n","    #   print(prompt, \"\\n\")\n","\n","    if use_index:\n","      importances_matrix, max_act, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=initial_max_act, initial_argmax=initial_max_index, scale_factor=scale_factor, return_all=True)\n","    else:\n","      importances_matrix, max_act, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=initial_max_act, scale_factor=scale_factor, return_all=True)\n","    info.append((importances_matrix, tokens_and_activations, max_index))\n","\n","  for prompt, activation, change in negative_prompts:\n","    if use_index:\n","      importances_matrix, max_act, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=initial_max_act, initial_argmax=initial_max_index, scale_factor=scale_factor, return_all=True)\n","    else:\n","      importances_matrix, max_act, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=initial_max_act, scale_factor=scale_factor, return_all=True)\n","    info.append((importances_matrix, tokens_and_activations, max_index))\n","\n","  return info\n","\n","\n","def fast_augment_and_visualise(model, layer, neuron, aug, pruned_prompt, use_index=False, **kwargs):\n","  tokens_and_importances, max_act, important_tokens, tokens_and_activations, initial_max_index = fast_measure_importance(model, layer, neuron, pruned_prompt)\n","\n","  positive_prompts, negative_prompts = augment(model, layer, neuron, pruned_prompt, aug, important_tokens=set(important_tokens), **kwargs)\n","  for i, (prompt, activation, change) in enumerate(positive_prompts):\n","    title = prompt\n","    if i == 0:\n","      title = \"Original - \" + prompt\n","    if use_index:\n","      tokens_and_importances, _, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=max_act, initial_argmax=initial_max_index)\n","    else:\n","      tokens_and_importances, _, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=max_act)\n","    # visualise(tokens_and_activations, tokens_and_importances, max_index, title=title, **kwargs)\n","\n","  for prompt, activation, change in negative_prompts:\n","    if use_index:\n","      tokens_and_importances, _, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=max_act, initial_argmax=initial_max_index)\n","    else:\n","      tokens_and_importances, _, _, tokens_and_activations, max_index = fast_measure_importance(model, layer, neuron, prompt, max_activation=max_act)\n","    # visualise(tokens_and_activations, tokens_and_importances, max_index, title=prompt, **kwargs)\n","\n","\n","def fast_run(model, layer, neuron, aug=fast_aug, snippets=None, num_examples=5, example_indexes=None, **kwargs):\n","  \"\"\"For a given neuron, grab the max activating dataset examples, run them through the pruning and augmentation steps, and visualise the results\"\"\"\n","  if snippets is None:\n","    snippets = get_snippets(model_name, layer, neuron)\n","    if example_indexes is not None:\n","      snippets = [snippet for i, snippet in enumerate(snippets) if i in example_indexes]\n","    else:\n","      snippets = snippets[:num_examples]\n","\n","  if isinstance(layer, int):\n","    layer = f\"blocks.{layer}.{layer_ending}\"\n","\n","  for snippet in snippets:\n","    pruned_prompt, _ = fast_prune(model, layer, neuron, snippet, include_post_context=False, **kwargs)\n","\n","    if pruned_prompt is None:\n","      continue\n","\n","    fast_augment_and_visualise(model, layer, neuron, aug, pruned_prompt, **kwargs)"],"metadata":{"id":"2aM3Yl-G5GvD","executionInfo":{"status":"ok","timestamp":1692652982914,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["def layer_and_neuron_to_index(layer, neuron, width=3072, block_size=None):\n","  index = (layer * width) + neuron\n","  if block_size is None:\n","    return index\n","  return divmod(index, block_size)\n","\n","def index_to_layer_and_neuron(index, width=3072):\n","  return divmod(index, width)"],"metadata":{"id":"SAEEnxw0642g","executionInfo":{"status":"ok","timestamp":1692652984598,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","\n","def evaluate(neuron_model, data, fire_threshold=0.5, **kwargs):\n","  y = []\n","  y_pred = []\n","  y_act = []\n","  y_pred_act = []\n","  for prompt_tokens, activations in data:\n","    # print(\"truth\")\n","    non_zero_indices = [i for i, activation in enumerate(activations) if activation > 0]\n","    start = max(0, non_zero_indices[0] - 10)\n","    end = min(len(prompt_tokens) - 1, non_zero_indices[-1] + 10)\n","    pred_activations = neuron_model.forward([prompt_tokens], return_activations=True)[0]\n","\n","    y_act.extend(activations)\n","    y_pred_act.extend(pred_activations)\n","\n","    important_context = list(zip(prompt_tokens, activations, pred_activations))[start:end]\n","\n","    # print(important_context)\n","    # print(len(pred_activations))\n","    pred_firings = [int(pred_activation >= fire_threshold) for pred_activation in pred_activations]\n","    firings = [int(activation >= fire_threshold) for activation in activations]\n","    y_pred.extend(pred_firings)\n","    y.extend(firings)\n","  # print(len(y), len(y_pred))\n","  print(classification_report(y, y_pred))\n","  report = classification_report(y, y_pred, output_dict=True)\n","\n","  y_act = np.array(y_act)\n","  y_pred_act = np.array(y_pred_act)\n","\n","  # y_pred_act = y_pred_act[y_act > 0.5]\n","  # y_act = y_act[y_act > 0.5]\n","\n","  # print(y_act[:10])\n","  # print(y_pred_act[:10])\n","\n","\n","  # y_pred_act = y_pred_act * np.mean(y_act) / np.mean(y_pred_act)\n","  # y_pred_act =\n","\n","  act_diff = y_pred_act - y_act\n","  mse = np.mean(np.power(act_diff, 2))\n","  variance = np.var(y_act)\n","  correlation = 1 - (mse / variance)\n","  # print(f\"{correlation=:.3f}, {mse=:.3f}, {variance=:.4f}\")\n","\n","  report[\"correlation\"] = correlation\n","  return report"],"metadata":{"id":"9VkXAgen7Nc4","executionInfo":{"status":"ok","timestamp":1692654291578,"user_tz":-60,"elapsed":609,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["def train_and_eval_baseline(model, layer, neuron, Baseline, train_proportion=0.5, fire_threshold=0.5, random_state=0, train_indexes=None, **kwargs):\n","  if isinstance(layer, int):\n","    layer = layer_index_to_name(layer)\n","\n","  layer_num = int(layer.split(\".\")[1])\n","\n","  base_max_act = float(activation_matrix[layer_num, neuron])\n","\n","  snippets = get_snippets(model_name, layer_num, neuron)\n","  # data = get_data(layer_num, neuron)\n","\n","  if train_indexes is None:\n","    train_snippets, test_snippets = train_test_split(snippets, train_size=train_proportion, random_state=random_state)\n","  else:\n","    train_snippets = [snippet for i, snippet in enumerate(snippets) if i in train_indexes]\n","    test_snippets = [snippet for i, snippet in enumerate(snippets) if i not in train_indexes]\n","  # train_data, test_data = train_test_split(data, train_size=train_proportion, random_state=0)\n","\n","  # train_data_snippets = [\"\".join(tokens) for tokens, activations in train_data if any(activation > fire_threshold for activation in activations)][:max_train_size]\n","  train_data_snippets = []\n","  all_train_snippets = train_snippets + train_data_snippets\n","\n","  baseline_model = Baseline(model, layer_num, neuron, **kwargs)\n","  baseline_model.fit(all_train_snippets)\n","\n","  print(\"Fitted model\")\n","\n","  # Not pruning so don't need to prepend_bos\n","  prepend_bos = False\n","\n","  max_test_data = []\n","  for snippet in test_snippets:\n","    tokens = model.to_tokens(snippet, prepend_bos=prepend_bos)\n","    str_tokens = model.to_str_tokens(snippet, prepend_bos=prepend_bos)\n","    logits, cache = model.run_with_cache(tokens)\n","    activations = cache[layer][0, :, neuron]\n","    max_test_data.append((str_tokens, activations.cpu() / base_max_act))\n","\n","  print(\"Max Activating Evaluation Data\")\n","  # try:\n","  stats = evaluate(baseline_model, max_test_data, fire_threshold=fire_threshold, **kwargs)\n","\n","  # except Exception as e:\n","  #   stats = {}\n","  #   print(f\"Stats failed with error: {e}\")\n","\n","  return stats"],"metadata":{"id":"xRzVubkF7T03","executionInfo":{"status":"ok","timestamp":1692652985409,"user_tz":-60,"elapsed":2,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","\n","def evaluate_baseline(baseline, folder_name, layers=6, neurons=3072, layer_start=0, neuron_start=0, **kwargs):\n","  random.seed(0)\n","\n","  all_neuron_indices = [i for i in range(neurons)]\n","\n","  all_stats = {}\n","  folder_path = os.path.join(base_path, f\"neuron_graphs/{model_name}/{folder_name}\")\n","\n","  if not os.path.exists(folder_path):\n","    print(\"Making\", folder_path)\n","    os.mkdir(folder_path)\n","\n","  if os.path.exists(f\"{folder_path}/stats.json\"):\n","    with open(f\"{folder_path}/stats.json\") as ifh:\n","      all_stats = json.load(ifh)\n","\n","  else:\n","    all_stats = {}\n","\n","  for i, layer in enumerate(range(layer_start, layers)):\n","    if layer not in all_stats:\n","      all_stats[layer] = {}\n","\n","    for j, neuron in enumerate(range(neuron_start, neurons)):\n","      print(f\"{layer=} {neuron=}\")\n","      try:\n","        stats = train_and_eval_baseline(model, layer, neuron, baseline, train_proportion=0.5, fire_threshold=0.5, **kwargs)\n","\n","        all_stats[layer][neuron] = stats\n","\n","        if j % 10 == 0:\n","          with open(f\"{folder_path}/stats.json\", \"w\") as ofh:\n","            json.dump(all_stats, ofh, indent=2)\n","\n","      except Exception as e:\n","        print(e)\n","        print(\"Failed\")\n","\n","  with open(f\"{folder_path}/stats.json\", \"w\") as ofh:\n","    json.dump(all_stats, ofh, indent=2)"],"metadata":{"id":"hiFMaZaF7lxy","executionInfo":{"status":"ok","timestamp":1692652985826,"user_tz":-60,"elapsed":2,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","import random\n","\n","\n","def get_summary_stats(path, verbose=True):\n","  summary_stats = []\n","  summary_stds = []\n","\n","  with open(path) as ifh:\n","    stats = json.load(ifh)\n","\n","  missing = 0\n","\n","  random.seed(0)\n","\n","  inelegible_count = 0\n","\n","  precision_case = 0\n","\n","  for layer, layer_stats in stats.items():\n","    # pprint(layer_stats)\n","    eligible_neurons = [neuron for neuron, neuron_stats in layer_stats.items() if \"1\" in neuron_stats]\n","    # neuron_sample = set(random.sample(eligible_neurons, 50))\n","    eligible_neurons = set(eligible_neurons)\n","\n","    aggr_stats_dict = {\"Inactivating\": defaultdict(list), \"Activating\": defaultdict(list)}\n","    for neuron, neuron_stats in layer_stats.items():\n","      if neuron not in eligible_neurons:\n","        inelegible_count += 1\n","        continue\n","\n","      aggr_stats_dict[\"Inactivating\"][\"Precision\"].append(neuron_stats[\"0\"][\"precision\"])\n","      aggr_stats_dict[\"Inactivating\"][\"Recall\"].append(neuron_stats[\"0\"][\"recall\"])\n","      aggr_stats_dict[\"Inactivating\"][\"F1\"].append(neuron_stats[\"0\"][\"f1-score\"])\n","\n","      # print(neuron_stats[\"0\"][\"precision\"], neuron_stats[\"0\"][\"recall\"], neuron_stats[\"0\"][\"f1-score\"],\n","      #       neuron_stats[\"1\"][\"precision\"], neuron_stats[\"1\"][\"recall\"], neuron_stats[\"1\"][\"f1-score\"])\n","\n","      # If we didn't predict anything as activating, treat this as 100% precision rather than 0%\n","      if neuron_stats[\"0\"][\"recall\"] == 1 and neuron_stats[\"1\"][\"recall\"] == 0:\n","        # print(\"Precision case\")\n","        precision_case += 1\n","        neuron_stats[\"1\"][\"precision\"] = 1.0\n","\n","      aggr_stats_dict[\"Activating\"][\"Precision\"].append(neuron_stats[\"1\"][\"precision\"])\n","      aggr_stats_dict[\"Activating\"][\"Recall\"].append(neuron_stats[\"1\"][\"recall\"])\n","      aggr_stats_dict[\"Activating\"][\"F1\"].append(neuron_stats[\"1\"][\"f1-score\"])\n","\n","    #   if neuron == \"20\":\n","    #     break\n","    # break\n","\n","\n","\n","      # if neuron_stats[\"1\"][\"recall\"] > 0.8:\n","      #   print(f'{layer}, {neuron}, {neuron_stats[\"1\"][\"precision\"]:.3f}, {neuron_stats[\"1\"][\"recall\"]:.3f}, {neuron_stats[\"1\"][\"f1-score\"]:.3f}')\n","    if verbose:\n","      print(\"Neurons Evaluated:\", len(aggr_stats_dict[\"Inactivating\"][\"Precision\"]))\n","\n","    avg_stats_dict = {\"Inactivating\": {}, \"Activating\": {}}\n","    std_stats_dict = {\"Inactivating\": {}, \"Activating\": {}}\n","    for token_type, inner_stats_dict in aggr_stats_dict.items():\n","      for stat_type, stat_arr in inner_stats_dict.items():\n","        avg_stats_dict[token_type][stat_type] = round(np.mean(stat_arr), 3)\n","        std_stats_dict[token_type][stat_type] = round(np.std(stat_arr), 3)\n","\n","    summary_stats.append(avg_stats_dict)\n","    summary_stds.append(std_stats_dict)\n","    # break\n","\n","  if verbose:\n","    for layer, (summary, std_summary) in enumerate(zip(summary_stats, summary_stds)):\n","      print(\"\\n\")\n","      pprint(summary)\n","      pprint(std_summary)\n","\n","    print(f\"{inelegible_count=}\")\n","    print(f\"{precision_case=}\")\n","\n","  return summary_stats"],"metadata":{"id":"yLsSZlW17nt-","executionInfo":{"status":"ok","timestamp":1692652986990,"user_tz":-60,"elapsed":2,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["### Graph Building"],"metadata":{"id":"qyRC9S6Q7aD-"}},{"cell_type":"code","source":["from collections import defaultdict, namedtuple, Counter\n","from itertools import zip_longest\n","import json\n","from graphviz import Digraph, Graph, escape\n","from typing import List, Dict\n","import os\n","from IPython.display import Image, display\n","\n","\n","class NeuronStore:\n","  def __init__(self, path):\n","    if not os.path.exists(path):\n","      neuron_store = {\n","          \"activating\": {},\n","          \"important\": {}\n","      }\n","      with open(path, \"w\") as ofh:\n","        json.dump(neuron_store, ofh, indent=2, ensure_ascii=False)\n","\n","    with open(path) as ifh:\n","      self.store = json.load(ifh)\n","\n","    self.to_sets()\n","    self.path = path\n","    self.count_tokens()\n","    self.by_neuron()\n","\n","  def save(self):\n","    self.to_lists()\n","    with open(self.path, \"w\") as ofh:\n","      json.dump(self.store, ofh, indent=2, ensure_ascii=False)\n","    self.to_sets()\n","\n","  def to_sets(self):\n","    self.store = {token_type: {token: set(info) for token, info in token_dict.items()} for token_type, token_dict in self.store.items()}\n","\n","  def to_lists(self):\n","    self.store = {token_type: {token: list(set(info)) for token, info in token_dict.items()} for token_type, token_dict in self.store.items()}\n","\n","  def by_neuron(self):\n","    self.neuron_to_tokens = {}\n","    for token_type, token_dict in self.store.items():\n","      for token, neurons in token_dict.items():\n","        for neuron in neurons:\n","          if neuron not in self.neuron_to_tokens:\n","            self.neuron_to_tokens[neuron] = {\"activating\": set(), \"important\": set()}\n","          self.neuron_to_tokens[neuron][token_type].add(token)\n","\n","  def search(self, tokens_and_types):\n","    match_arr = []\n","\n","    for token, token_type in tokens_and_types:\n","      token_types = [token_type] if token_type is not None else [\"activating\", \"important\"]\n","      token_matches = set()\n","\n","      for token_type in token_types:\n","        matches = self.store[token_type].get(token, set())\n","        token_matches |= matches\n","\n","      match_arr.append(token_matches)\n","\n","    valid_matches = set.intersection(*match_arr)\n","    return valid_matches\n","\n","  def count_tokens(self):\n","    self.neuron_individual_token_counts = defaultdict(Counter)\n","    self.neuron_total_token_counts = Counter()\n","    for token_type, token_dict in self.store.items():\n","      for token, neurons in token_dict.items():\n","        for neuron in neurons:\n","          self.neuron_individual_token_counts[neuron][token] += 1\n","          self.neuron_total_token_counts[neuron] += 1\n","\n","  def find_similar(self, target_token_types=None, threshold=0.9):\n","    if target_token_types is None:\n","      target_token_types = {\"activating\", \"important\"}\n","\n","    similar_pairs = []\n","    subset_pairs = []\n","\n","    for i, (neuron_1, neuron_dict_1) in enumerate(self.neuron_to_tokens.items()):\n","      if i % 1000 == 0:\n","        print(f\"{i} of {len(self.neuron_to_tokens.items())} complete\")\n","\n","      for j, (neuron_2, neuron_dict_2) in enumerate(self.neuron_to_tokens.items()):\n","        if i <= j:\n","          continue\n","\n","        all_similar = []\n","        all_subset = []\n","\n","        for token_type in target_token_types:\n","          length_1 = len(neuron_dict_1[token_type])\n","          length_2 = len(neuron_dict_2[token_type])\n","\n","          intersection = neuron_dict_1[token_type] & neuron_dict_2[token_type]\n","          similar = (len(intersection) / max(length_1, length_2, 1)) >= threshold\n","          subset = len(intersection) / max(min(length_1, length_2), 1) >= threshold\n","\n","          all_similar.append(similar)\n","          all_subset.append(subset)\n","\n","        if all(all_similar):\n","          similar_pairs.append((neuron_1, neuron_2))\n","        elif all(all_subset):\n","          # The first token indicates the superset neuron and the second the subset neuron\n","          subset_pair = (neuron_1, neuron_2) if length_2 < length_1 else (neuron_2, neuron_1)\n","          subset_pairs.append(subset_pair)\n","\n","    return similar_pairs, subset_pairs\n","\n","\n","test_neuron_store = NeuronStore(f\"{base_path}/data/neuron_store_{model_name}_test.json\")\n","\n","\n","def view_neuron(path):\n","  display(Image(filename=path))\n","\n","\n","class NeuronNode:\n","  def __init__(self, id_=None, value=None, children=None, depth=None, important=False, activator=False):\n","    if value is None:\n","      value = {}\n","    if children is None:\n","      children = {}\n","    self.id_ = id_\n","    self.value = value\n","    self.children = children\n","    self.depth = depth\n","\n","  def __repr__(self):\n","    return f\"ID: {self.id_}, Value: {json.dumps(self.value)}\"\n","\n","  def paths(self):\n","    if not self.children:\n","      return [[self.value]]  # one path: only contains self.value\n","    paths = []\n","    for child_token, child_tuple in self.children.items():\n","      child_node, _ = child_tuple\n","      for path in child_node.paths():\n","          paths.append([self.value] + path)\n","    return paths\n","\n","\n","class NeuronEdge:\n","  def __init__(self, weight=0, parent=None, child=None):\n","    self.weight = weight\n","    self.parent = parent\n","    self.child = child\n","\n","  def __repr__(self):\n","    parent_str = json.dumps(self.parent.id_) if self.parent is not None else \"None\"\n","    child_str = json.dumps(self.child.id_) if self.child is not None else \"None\"\n","    return f\"Weight: {self.weight:.3f}\\nParent: {parent_str}\\nChild: {child_str}\"\n","\n","\n","class NeuronModel:\n","  def __init__(self, layer, neuron, activation_threshold=0.1, importance_threshold=0.5, folder_name=None, neuron_store=None, **kwargs):\n","    self.layer = layer\n","    self.neuron = neuron\n","    self.Element = namedtuple(\"Element\", \"importance, activation, token, important, activator, ignore, is_end, token_value\")\n","    self.neuron_store = neuron_store\n","\n","    self.root_token = \"**ROOT**\"\n","    self.ignore_token = \"**IGNORE**\"\n","    self.end_token = \"**END**\"\n","    self.special_tokens = {self.root_token, self.ignore_token, self.end_token}\n","\n","    self.root = (NeuronNode(-1, self.Element(0, 0, self.root_token, False, False, True, False, self.root_token), depth=-1), NeuronEdge())\n","    self.trie_root = (NeuronNode(-1, self.Element(0, 0, self.root_token, False, False, True, False, self.root_token), depth=-1), NeuronEdge())\n","    self.activation_threshold = activation_threshold\n","    self.importance_threshold = importance_threshold\n","    # self.net = Network(notebook=True)\n","    # self.net = Graph(graph_attr={\"rankdir\": \"LR\", \"splines\": \"spline\", \"ranksep\": \"20\", \"nodesep\": \"1\"}, node_attr={\"fixedsize\": \"true\", \"width\": \"1.5\"})\n","    # self.net = Graph(\n","    #     graph_attr={\"rankdir\": \"RL\", \"splines\": \"spline\", \"ranksep\": \"5\", \"nodesep\": \"1\"},\n","    #     node_attr={\"fixedsize\": \"true\", \"width\": \"2\"}\n","    # )\n","    # self.net = Graph(\n","    #     graph_attr={\"rankdir\": \"RL\", \"splines\": \"spline\", \"ranksep\": \"2\", \"nodesep\": \"0.25\"},\n","    #     node_attr={\"fixedsize\": \"true\", \"width\": \"2\", \"height\": \"0.75\"}\n","    # )\n","    self.net = Digraph(\n","        graph_attr={\"rankdir\": \"RL\", \"splines\": \"spline\", \"ranksep\": \"1.5\", \"nodesep\": \"0.2\"},\n","        node_attr={\"fixedsize\": \"true\", \"width\": \"2\", \"height\": \"0.75\"}\n","    )\n","    self.node_count = 0\n","    self.trie_node_count = 0\n","    self.max_depth = 0\n","    self.folder_name = folder_name\n","\n","  def __call__(self, tokens_arr: List[List[str]]) -> List[List[float]]:\n","    return self.forward(tokens_arr)\n","\n","  def fit(self, data):\n","    for example_data in data:\n","      for j, info in enumerate(example_data):\n","        if j == 0:\n","          lines, important_index_sets = self.make_line(info)\n","        else:\n","          lines, _ = self.make_line(info, important_index_sets)\n","\n","        for line in lines:\n","          # print(\"\\nline\", line)\n","          self.add(self.root, line, graph=True)\n","          self.add(self.trie_root, line, graph=False)\n","\n","    # print(\"Paths before merge\")\n","    # for path in self.trie_root[0].paths():\n","    #   print(path)\n","\n","    self.build(self.root)\n","    self.merge_ignores()\n","\n","    self.save_neurons()\n","\n","    print(\"Paths after merge\")\n","    paths = []\n","    for path in self.trie_root[0].paths():\n","      # print(path)\n","      paths.append(path)\n","\n","    return paths\n","\n","  def save_neurons(self):\n","    visited = set() # List to keep track of visited nodes.\n","    queue = []      # Initialize a queue\n","\n","    visited.add(self.trie_root[0].id_)\n","    queue.append(self.trie_root)\n","\n","    while queue:\n","      node, edge = queue.pop(0)\n","\n","      token = node.value.token\n","\n","      if token not in self.special_tokens:\n","        add_dict = self.neuron_store.store[\"activating\"] if node.value.activator else self.neuron_store.store[\"important\"]\n","        if token not in add_dict:\n","          add_dict[token] = set()\n","        add_dict[token].add(f\"{self.layer}_{self.neuron}\")\n","\n","      for token, neighbour in node.children.items():\n","        new_node, new_edge = neighbour\n","        if new_node.id_ not in visited:\n","          visited.add(new_node.id_)\n","          queue.append(neighbour)\n","\n","  @staticmethod\n","  def normalise(token):\n","      normalised_token = token.lower() if token.istitle() and len(token) > 1 else token\n","      normalised_token = normalised_token.strip() if len(normalised_token) > 1 and any(c.isalpha() for c in normalised_token) else normalised_token\n","      return normalised_token\n","\n","  def make_line(self, info, important_index_sets=None):\n","    if important_index_sets is None:\n","      important_index_sets = []\n","      create_indices = True\n","    else:\n","      create_indices = False\n","\n","    importances_matrix, tokens_and_activations, max_index = info\n","\n","    # print(tokens_and_activations)\n","\n","    all_lines = []\n","\n","    for i, (token, activation) in enumerate(tokens_and_activations):\n","      if create_indices:\n","        important_index_sets.append(set())\n","\n","      # if activation > 0.2:\n","      #   print([token], activation)\n","\n","      if not activation > self.activation_threshold:\n","        continue\n","\n","      # print(\"\\ntoken\", token)\n","\n","      before = tokens_and_activations[:i + 1]\n","\n","      line = []\n","      last_important = 0\n","\n","      if not create_indices:\n","        # The if else is a bit of a hack to account for augmentations that have a different number of tokens to the original prompt\n","        important_indices = important_index_sets[i] if i < len(important_index_sets) else important_index_sets[-1]\n","      else:\n","        important_indices = set()\n","\n","      # print(\"before\", before)\n","\n","      for j, (seq_token, seq_activation) in enumerate(reversed(before)):\n","        if seq_token == \"<|endoftext|>\":\n","          continue\n","\n","        seq_index = len(before) - j - 1\n","        # Stop when we reach the last matrix entry, which corresponds to the last activating token\n","        # if seq_index >= len(importances_matrix):\n","        #   break\n","        important_token, importance = importances_matrix[seq_index, i]\n","        importance = float(importance)\n","        # print(\"importance\", importance)\n","\n","        important = importance > self.importance_threshold or (not create_indices and seq_index in important_indices)\n","        activator = seq_activation > self.activation_threshold\n","\n","        # print(\"important_index_sets[i]\", important_index_sets[i])\n","        # print(\"create_indices\", create_indices)\n","        # print(\"important\", important)\n","        # print(\"seq_token\", seq_token)\n","        # print(\"seq_index\", seq_index)\n","        # print(\"important_token\", important_token)\n","\n","        if important and create_indices:\n","          important_indices.add(seq_index)\n","          # print(\"important_indices\", important_indices)\n","\n","        ignore = not important and j != 0\n","        is_end = False\n","\n","        seq_token_identifier = self.ignore_token if ignore else seq_token\n","\n","        new_element = self.Element(importance, seq_activation, seq_token_identifier, important, activator, ignore, is_end, seq_token)\n","\n","        # print(\"new_element\", new_element)\n","\n","        if not ignore:\n","          last_important = j\n","\n","        line.append(new_element)\n","\n","      line = line[:last_important + 1]\n","      # Add an end node\n","      line.append(self.Element(0, activation, self.end_token, False, False, True, True, self.end_token))\n","      # print(line)\n","      all_lines.append(line)\n","\n","      if create_indices:\n","        important_index_sets[i] = important_indices\n","\n","    # print(\"From\", tokens_and_activations)\n","    # for line in all_lines:\n","    #   print(\"\\nMade\", line)\n","\n","    return all_lines, important_index_sets\n","\n","  def add(self, start_tuple, line, graph=True):\n","    current_tuple = start_tuple\n","    previous_element = None\n","    important_count = 0\n","\n","    # print(\"starting at\", current_tuple)\n","    # print(\"adding\", line)\n","\n","    start_depth = current_tuple[0].depth\n","\n","    for i, element in enumerate(line):\n","      # print(\"\\nelement\", element)\n","      if element is None and i > 0:\n","        break\n","\n","      # importance, activation, token, important, activator, ignore, is_end = element\n","\n","      if element.ignore and graph:\n","        continue\n","\n","      # Normalise token\n","      element = element._replace(token=self.normalise(element.token))\n","\n","      if graph:\n","        # Set end value as we don't have end nodes in the graph\n","        # The current node is an end if there's only one more node, as that will be the end node that we don't add\n","        is_end = i == len(line) - 2\n","        element = element._replace(is_end=is_end)\n","\n","      important_count += 1\n","\n","      current_node, current_edge = current_tuple\n","\n","      if not current_node.value.ignore:\n","        prev_important_node = current_node\n","\n","      # print(\"current_node\", current_node)\n","      # print(\"children\", current_node.children)\n","\n","      if element.token in current_node.children:\n","        current_tuple = current_node.children[element.token]\n","        # print(\"Already in children\")\n","        continue\n","\n","      # if i == 0:\n","      #   weight = 0\n","      # # elif i == 1:\n","      #   # weight = previous_element.value[\"activation\"] * element.value[\"importance\"]\n","      # else:\n","      #   weight = prev_important_node.value.importance * element.importance\n","      weight = 0\n","\n","      depth = start_depth + important_count\n","      new_node = NeuronNode(self.node_count, element, {}, depth=depth)\n","      new_tuple = (new_node, NeuronEdge(weight, current_node, new_node))\n","\n","      self.max_depth = depth if depth > self.max_depth else self.max_depth\n","      # print(current_node)\n","      # print(new_node)\n","\n","      current_node.children[element.token] = new_tuple\n","\n","      # print(\"Added new node\")\n","      # print(\"children\", current_node.children)\n","\n","      current_tuple = new_tuple\n","\n","      self.node_count += 1\n","\n","    return current_tuple\n","\n","  # def merge(self, parent_tuple, merge_tuple):\n","  #   visited = set() # List to keep track of visited nodes.\n","  #   queue = []      # Initialize a queue\n","\n","  #   visited.add(merge_tuple[0].id_)\n","  #   queue.append(merge_tuple)\n","\n","  #   while queue:\n","  #     node, edge = queue.pop(0)\n","\n","  #     parent_node, _ = parent_tuple\n","\n","  #     parent_tuple = self.add(parent_node, [node.value])\n","\n","  #     for token, neighbour in node.children.items():\n","  #       new_node, new_edge = neighbour\n","  #       if new_node.id_ not in visited:\n","  #         visited.add(new_node.id_)\n","  #         queue.append(neighbour)\n","\n","  def merge_ignores(self):\n","    \"\"\"\n","    Where a set of children contain an ignore token, merge the other nodes into it:\n","      - Fully merge if the other node is not an end node\n","      - Give the ignore node the other node's children (if it has any) if the other node is an end node\n","    \"\"\"\n","    # print(\"\\n\\n******MERGING*******\")\n","    visited = set() # List to keep track of visited nodes.\n","    queue = []      # Initialize a queue\n","\n","    visited.add(self.trie_root[0].id_)\n","    queue.append(self.trie_root)\n","\n","    while queue:\n","      node, edge = queue.pop(0)\n","\n","      token = node.value.token\n","\n","      # print(node)\n","\n","      if self.ignore_token in node.children:\n","        ignore_tuple = node.children[self.ignore_token]\n","\n","        # print(\"ignore_tuple\", ignore_tuple)\n","\n","        to_remove = []\n","\n","        for child_token, child_tuple in node.children.items():\n","          if child_token == self.ignore_token:\n","            continue\n","\n","          child_node, child_edge = child_tuple\n","\n","          child_paths = child_node.paths()\n","\n","          for path in child_paths:\n","            # print(\"path\", path)\n","            # Don't merge if the path is only the first tuple, or the first tuple and an end tuple\n","            if len(path) <= 1 or (len(path) == 2 and path[-1].token == self.end_token):\n","              continue\n","            # Merge the path (not including the first tuple that we're merging)\n","            self.add(ignore_tuple, path[1:], graph=False)\n","\n","          # Add the node to a list to be removed later if it isn't an end node and doesn't have an end node in its children\n","          if not child_node.value.is_end and not self.end_token in child_node.children:\n","          # if not self.end_token in child_node.children:\n","            to_remove.append(child_token)\n","\n","        for child_token in to_remove:\n","          node.children.pop(child_token)\n","\n","      for token, neighbour in node.children.items():\n","        new_node, new_edge = neighbour\n","        if new_node.id_ not in visited:\n","          visited.add(new_node.id_)\n","          queue.append(neighbour)\n","\n","  def search(self, tokens: List[str]) -> float:\n","    \"\"\"Evaluate the activation on the first token in tokens\"\"\"\n","    current_tuple = self.trie_root\n","\n","    # print(\"\\n\")\n","    activations = [0]\n","\n","    for i, token in enumerate(reversed(tokens)):\n","      token = self.normalise(token)\n","\n","      current_node, current_edge = current_tuple\n","\n","      # print(\"i, token\", i, [token])\n","      # print(\"current_node.children\", current_node.children)\n","\n","      if token in current_node.children or self.ignore_token in current_node.children:\n","        current_tuple = current_node.children[token] if token in current_node.children else current_node.children[self.ignore_token]\n","\n","        node, edge = current_tuple\n","        # If the first token is not an activator, return early\n","        if i == 0:\n","          if not node.value.activator:\n","            break\n","          activation = node.value.activation\n","\n","        # print(\"node\", node)\n","\n","        if self.end_token in node.children:\n","          # debug(\"Returning\", activation)\n","          end_node, _ = node.children[self.end_token]\n","          end_activation = end_node.value.activation\n","          activations.append(end_activation)\n","\n","      else:\n","        break\n","\n","    # Return the activation on the longest sequence\n","    return activations[-1]\n","\n","  def forward(self, tokens_arr: List[List[str]], return_activations=True) -> List[List[float]]:\n","    if isinstance(tokens_arr[0], str):\n","      raise ValueError(f\"tokens_arr must be of type List[List[str]]\")\n","\n","    # print(\"\\n\\n******PROCESSING*******\")\n","    # print(tokens_arr)\n","    \"\"\"Evaluate the activation on each token in some input tokens\"\"\"\n","    all_activations = []\n","    all_firings = []\n","\n","    for tokens in tokens_arr:\n","      activations = []\n","      firings = []\n","\n","      for j in range(len(tokens)):\n","        token_activation = self.search(tokens[:len(tokens) - j])\n","        activations.append(token_activation)\n","        firings.append(token_activation > self.activation_threshold)\n","\n","      activations = list(reversed(activations))\n","      firings = list(reversed(firings))\n","\n","      all_activations.append(activations)\n","      all_firings.append(firings)\n","\n","      # print(list(zip(tokens, activations)))\n","\n","    if return_activations:\n","      return all_activations\n","    return all_firings\n","\n","  def build(self, start_node, graph=True):\n","    \"\"\"Build a graph to visualise\"\"\"\n","    # print(\"\\n\\n******BUILDING*******\")\n","    visited = set() # List to keep track of visited nodes.\n","    queue = []     #Initialize a queue\n","\n","    visited.add(start_node[0].id_)\n","    queue.append(start_node)\n","\n","    zero_width = u'\\u200b'\n","    # zero_width = \"a\"\n","\n","    tokens_by_layer = {}\n","    node_id_to_graph_id = {}\n","    token_by_layer_count = defaultdict(Counter)\n","    added_ids = set()\n","    node_count = 0\n","    depth_to_subgraph = {}\n","    added_edges = set()\n","\n","    node_edge_tuples = []\n","\n","    adjust = lambda x, y: (x - y) / (1 - y)\n","\n","    while queue:\n","      # print(queue)\n","      node, edge = queue.pop(0)\n","\n","      node_edge_tuples.append((node, edge))\n","\n","      for token, neighbour in node.children.items():\n","        # print(\"token\", token)\n","        new_node, new_edge = neighbour\n","        if new_node.id_ not in visited:\n","          visited.add(new_node.id_)\n","          queue.append(neighbour)\n","\n","    for node, edge in node_edge_tuples:\n","      token = node.value.token\n","      depth = node.depth\n","\n","      # if token == \"\":\n","      #   continue\n","\n","      if depth not in tokens_by_layer:\n","        tokens_by_layer[depth] = {}\n","        # depth_to_subgraph[depth] = Graph(name=f\"cluster_{str(self.max_depth - depth)}\")\n","        depth_to_subgraph[depth] = Digraph(name=f\"cluster_{str(self.max_depth - depth)}\")\n","        # depth_to_subgraph[depth].attr(label=f\"Depth {str(depth)}\")\n","        depth_to_subgraph[depth].attr(pencolor=\"white\", penwidth=\"3\")\n","\n","      token_by_layer_count[depth][token] += 1\n","\n","      if not graph:\n","        # This is a horrible hack to allow us to have a dict with the \"same\" token as multiple keys - by adding zero width spaces the tokens look the same but are actually different. This allows us to display a trie rather than a node-collapsed graph\n","        seen_count = token_by_layer_count[depth][token] - 1\n","        add = zero_width * seen_count\n","        token += add\n","\n","      if token not in tokens_by_layer[depth]:\n","        tokens_by_layer[depth][token] = str(node_count)\n","        node_count += 1\n","\n","      graph_node_id = tokens_by_layer[depth][token]\n","      node_id_to_graph_id[node.id_] = graph_node_id\n","\n","    # for node, edge in reversed(node_edge_tuples):\n","    #   token = node.value.token\n","    #   depth = node.depth\n","\n","      # graph_node_id = tokens_by_layer[depth][token]\n","      # node_id_to_graph_id[node.id_] = graph_node_id\n","      current_graph = depth_to_subgraph[depth]\n","\n","      if depth == 0:\n","        # colour red according to activation for depth 0 tokens\n","        scaled_activation = int(adjust(node.value.activation, max(0, self.activation_threshold - 0.2)) * 255)\n","        rgb = (255, 255 - scaled_activation, 255 - scaled_activation)\n","      else:\n","        # colour blue according to importance for all other tokens\n","        # Shift and scale importance so the importance threshold becomes 0\n","\n","        scaled_importance = int(adjust(node.value.importance, max(0.1, self.importance_threshold - 0.2)) * 255)\n","        rgb = (255 - scaled_importance, 255 - scaled_importance, 255)\n","\n","      hex = \"#{0:02x}{1:02x}{2:02x}\".format(*self.clamp(rgb))\n","\n","      # self.net.add_node(node.id_, label=node.value[\"token\"], color=hex)\n","\n","      if graph_node_id not in added_ids and not node.value.ignore:\n","        display_token = token.strip(zero_width)\n","        display_token = json.dumps(display_token).strip('[]\"') if '\"' not in token else display_token\n","        if set(display_token) == {\" \"}:\n","          display_token = f\"'{display_token}'\"\n","\n","        # self.net.node(graph_node_id, node.value[\"token\"])\n","        # print(\"token\", token, escape(token))\n","        fontcolor = \"white\" if depth != 0 and rgb[1] < 130 else \"black\"\n","        fontsize = \"25\" if len(display_token) < 12 else \"18\"\n","        edge_width = \"7\" if node.value.is_end else \"3\"\n","        # current_graph.node(\n","        #     graph_node_id, f\"< <B> {escape(token)} </B> >\", fillcolor=hex, shape=\"box\",\n","        #     style=\"filled,solid\", fontcolor=fontcolor, fontsize=fontsize,\n","        #     penwidth=edge_width\n","        # )\n","\n","        current_graph.node(\n","            graph_node_id, f\"{escape(display_token)}\", fillcolor=hex, shape=\"box\",\n","            style=\"filled,solid\", fontcolor=fontcolor, fontsize=fontsize,\n","            penwidth=edge_width\n","        )\n","        added_ids.add(graph_node_id)\n","\n","      if edge.parent is not None and edge.parent.id_ in visited and not edge.parent.value.ignore:\n","        # self.net.add_edge(node.id_, edge.parent.id_, value=edge.weight, title=round(edge.weight, 2))\n","        # pprint(node_id_to_graph_id)\n","        # print([token])\n","        # print([edge.parent.value.token])\n","        # print([edge.parent.value.importance])\n","        graph_parent_id = node_id_to_graph_id[edge.parent.id_]\n","        # current_graph.edge(graph_parent_id, graph_node_id, constraint='false')\n","        edge_tuple = (graph_parent_id, graph_node_id)\n","        if edge_tuple not in added_edges:\n","          self.net.edge(*edge_tuple, penwidth=\"3\", dir=\"back\")\n","          added_edges.add(edge_tuple)\n","\n","      # print(\"node\", node)\n","      # print(\"edge\", edge)\n","      # print(\"node.children\", node.children)\n","\n","      # for token, neighbour in node.children.items():\n","      #   # print(\"token\", token)\n","      #   new_node, new_edge = neighbour\n","      #   if new_node.id_ not in visited:\n","      #     visited.add(new_node.id_)\n","      #     queue.append(neighbour)\n","\n","    for depth, subgraph in depth_to_subgraph.items():\n","      self.net.subgraph(subgraph)\n","\n","    path_parts = ['neuron_graphs', model_name]\n","\n","    if self.folder_name is not None:\n","      path_parts.append(self.folder_name)\n","\n","    path_parts.append(f\"{self.layer}_{self.neuron}\")\n","\n","    save_path = base_path\n","    for path_part in path_parts:\n","      save_path += f\"/{path_part}\"\n","      if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","\n","    self.net.format = 'svg'\n","    filename = \"graph\" if graph else \"trie\"\n","    self.net.render(f\"{save_path}/{filename}\", view=False)\n","    self.net.format = 'png'\n","    self.net.render(f\"{save_path}/{filename}\", view=False)\n","    # print(self.net.source)\n","\n","  @staticmethod\n","  def clamp(arr):\n","    return [max(0, min(x, 255)) for x in arr]"],"metadata":{"id":"VaN1-9PS7Zey","executionInfo":{"status":"ok","timestamp":1692652988000,"user_tz":-60,"elapsed":516,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","class TokenPredictor:\n","  def __init__(self, model, layer, neuron, activation_threshold=0.5):\n","    self.model = model\n","    self.layer = layer\n","    self.neuron = neuron\n","    self.activation_threshold = activation_threshold\n","\n","    self.layer_name = layer_index_to_name(layer)\n","    self.max_activation = activation_matrix[layer, neuron]\n","\n","  def fit(self, texts):\n","    prepend_bos = False\n","\n","    self.token_to_activations = defaultdict(list)\n","    for i, text in enumerate(texts):\n","      all_tokens = model.to_tokens(text, prepend_bos=prepend_bos)\n","      logits, cache = model.run_with_cache(all_tokens)\n","      neuron_activations = cache[self.layer_name][0, :, self.neuron]\n","\n","      tokens = model.to_str_tokens(text, prepend_bos=prepend_bos)\n","      neuron_activations = neuron_activations.to(\"cpu\")\n","      for token, activation in zip(tokens, neuron_activations):\n","        activation = activation.item()\n","        self.token_to_activations[token].append(activation / self.max_activation)\n","\n","    self.token_to_activation = {token: np.max(activations) for token, activations in self.token_to_activations.items()}\n","\n","  def forward(self, tokens_arr: List[List[str]], return_activations=True) -> List[List[float]]:\n","    all_activations = []\n","    all_firings = []\n","\n","    for tokens in tokens_arr:\n","      activations = []\n","      firings = []\n","\n","      for token in tokens:\n","        activation = self.token_to_activation.get(token, 0)\n","\n","        activations.append(activation)\n","        firings.append(activation > self.activation_threshold)\n","\n","      all_activations.append(activations)\n","      all_firings.append(firings)\n","\n","    if return_activations:\n","      return all_activations\n","    return all_firings"],"metadata":{"id":"Sz1f9NT-7gWR","executionInfo":{"status":"ok","timestamp":1692652988523,"user_tz":-60,"elapsed":2,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","class NGramBaseline:\n","  def __init__(self, model, layer, neuron, prior_context=1, activation_threshold=0.5):\n","    self.model = model\n","    self.layer = layer\n","    self.neuron = neuron\n","    self.activation_threshold = activation_threshold\n","    self.prior_context = prior_context\n","\n","    self.layer_name = layer_index_to_name(layer)\n","    self.max_activation = activation_matrix[layer, neuron]\n","\n","  def fit(self, texts):\n","    prepend_bos = False\n","\n","    self.seq_to_activations = defaultdict(list)\n","    self.activating_tokens = set()\n","\n","    for i, text in enumerate(texts):\n","      all_tokens = model.to_tokens(text, prepend_bos=prepend_bos)\n","      logits, cache = model.run_with_cache(all_tokens)\n","      neuron_activations = cache[self.layer_name][0, :, self.neuron].cpu()\n","\n","      tokens = model.to_str_tokens(text, prepend_bos=prepend_bos)\n","      for j, (token, activation) in enumerate(zip(tokens, neuron_activations)):\n","        activation = activation.item()\n","        if activation < self.activation_threshold:\n","          continue\n","        token_seq = tokens[max(0, j - self.prior_context):j + 1]\n","        # print(token_seq, activation)\n","        self.activating_tokens.add(token)\n","        self.seq_to_activations[\"\".join(token_seq)].append(activation / self.max_activation)\n","\n","    self.seq_to_activation = {seq: np.max(activations) for seq, activations in self.seq_to_activations.items()}\n","\n","    # pprint(self.seq_to_activation)\n","\n","  def forward(self, tokens_arr: List[List[str]], return_activations=True) -> List[List[float]]:\n","    all_activations = []\n","    all_firings = []\n","\n","    for tokens in tokens_arr:\n","      activations = []\n","      firings = []\n","\n","      for j, token in enumerate(tokens):\n","        if token not in self.activating_tokens:\n","          activations.append(0)\n","          firings.append(0 > self.activation_threshold)\n","          continue\n","\n","        token_seq = tokens[max(0, j - self.prior_context):j + 1]\n","        activation = self.seq_to_activation.get(\"\".join(token_seq), 0)\n","\n","        activations.append(activation)\n","        firings.append(activation > self.activation_threshold)\n","\n","      all_activations.append(activations)\n","      all_firings.append(firings)\n","\n","    if return_activations:\n","      return all_activations\n","    return all_firings"],"metadata":{"id":"JawhSQv97g0J","executionInfo":{"status":"ok","timestamp":1692652988932,"user_tz":-60,"elapsed":2,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["## Run"],"metadata":{"id":"r2SDxoM97tQ9"}},{"cell_type":"code","source":["if not os.path.exists(f\"{base_path}/neuron_graphs\"):\n","  os.mkdir(\"neuron_graphs\")"],"metadata":{"id":"L11FXsKQ8XYU","executionInfo":{"status":"ok","timestamp":1692652989963,"user_tz":-60,"elapsed":2,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def run_training(layers, neurons, folder_name, sample_num=None, params=None):\n","  if params is None or not params:\n","    params = {\n","        \"importance_threshold\": 0.75,\n","        \"n\": 5,\n","        \"max_train_size\": None,\n","        \"train_proportion\": 0.5,\n","        \"max_eval_size\": 0.5,\n","        \"activation_threshold\": 0.5,\n","        \"token_activation_threshold\": 1,\n","        \"fire_threshold\": 0.5\n","    }\n","  print(f\"{params=}\\n\")\n","  random.seed(0)\n","\n","  all_neuron_indices = [i for i in range(neurons)]\n","\n","  if not os.path.exists(f\"{base_path}/neuron_graphs/{model_name}\"):\n","    os.mkdir(f\"{base_path}/neuron_graphs/{model_name}\")\n","\n","  neuron_store = NeuronStore(f\"{base_path}/neuron_graphs/{model_name}/neuron_store.json\")\n","\n","  folder_path = os.path.join(base_path, f\"neuron_graphs/{model_name}/{folder_name}\")\n","\n","  if not os.path.exists(folder_path):\n","    print(\"Making\", folder_path)\n","    os.mkdir(folder_path)\n","\n","  if os.path.exists(f\"{folder_path}/stats.json\"):\n","    with open(f\"{folder_path}/stats.json\") as ifh:\n","      all_stats = json.load(ifh)\n","\n","  else:\n","    all_stats = {}\n","\n","\n","  for layer in range(layers):\n","    if sample_num is None:\n","      chosen_neuron_indices = all_neuron_indices\n","    else:\n","      chosen_neuron_indices = random.sample(all_neuron_indices, sample_num)\n","      chosen_neuron_indices = sorted(chosen_neuron_indices)\n","\n","    all_stats[layer] = {}\n","    for i, neuron in enumerate(chosen_neuron_indices):\n","      print(f\"{layer=} {neuron=}\")\n","      try:\n","        stats = train_and_eval(model, layer, neuron, folder_name=folder_name, neuron_store=neuron_store, **params)\n","\n","        all_stats[layer][neuron] = stats\n","\n","        if i % 10 == 0:\n","          neuron_store.save()\n","          with open(f\"{folder_path}/stats.json\", \"w\") as ofh:\n","            json.dump(all_stats, ofh, indent=2)\n","\n","      except Exception as e:\n","        print(e)\n","        print(\"Failed\")\n","\n","  neuron_store.save()\n","  with open(f\"{folder_path}/stats.json\", \"w\") as ofh:\n","    json.dump(all_stats, ofh, indent=2)"],"metadata":{"id":"WQNyiRdR7vQi","executionInfo":{"status":"ok","timestamp":1692655001014,"user_tz":-60,"elapsed":486,"user":{"displayName":"Alex Foote","userId":"09524383492218707220"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["Change any params needed (if you don't specify it will use good defaults), set the number of layers and neurons for your model (see https://neuroscope.io/index.html), specify the name of the folder to save to (this will be made automatically) and specify how many neurons to run on by setting `sample_num` (if you set it to `None` it will run on all neurons)\n","\n","This will run and evaluate the model, and save the neuron graphs to the folder specified"],"metadata":{"id":"g3JSvLwk85cV"}},{"cell_type":"code","source":["%%time\n","params = {}\n","\n","folder_name = \"test_solu\"\n","\n","run_training(\n","    layers=6,\n","    neurons=3072,\n","    folder_name=folder_name,\n","    sample_num=1,\n","    params=params\n",")"],"metadata":{"id":"csn9cpta7ioA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the averaged stats for the model"],"metadata":{"id":"d_Oz5aTP9p46"}},{"cell_type":"code","source":["get_summary_stats(f\"{base_path}/neuron_graphs/{model_name}/{folder_name}/stats.json\")"],"metadata":{"id":"PyuDdxVsJKoR"},"execution_count":null,"outputs":[]}]}